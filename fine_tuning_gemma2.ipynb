{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "### 1) How to Fine-tune Gemma 2 for Advanced Reasoning in Communication, Translation and Multilingual Tasks\n",
    "\n",
    "Large Language Models (LLMs) like Gemma 2 have shown remarkable capabilities, but they can struggle with complex translation and cross-lingual communication tasks that require nuanced reasoning. Traditional fine-tuning with Chain-of-Thought (CoT) provides some improvements but has inherent limitations when dealing with multilingual scenarios.\n",
    "\n",
    "In this tutorial, we'll explore an innovative approach to enhance Gemma 2's reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm introduced by [Hao et al. (2024)](https://arxiv.org/pdf/2412.06769). Instead of constraining the model to reason in language space, we'll leverage continuous latent representations to enable more flexible and powerful reasoning patterns, particularly beneficial for translation and cross-lingual tasks.\n",
    "\n",
    "By utilizing the last hidden state as a \"continuous thought\" and feeding it back directly as input embeddings, we can help the model develop more sophisticated reasoning strategies. This approach allows Gemma 2 to:\n",
    "\n",
    "- Explore multiple reasoning paths simultaneously\n",
    "- Avoid premature commitment to single translations\n",
    "- Handle complex linguistic nuances more effectively\n",
    "- Reduce token overhead during inference\n",
    "\n",
    "This tutorial will guide you through implementing this cutting-edge fine-tuning approach using a real-world dataset, helping you transform Gemma 2 into a more capable multilingual reasoning system.\n",
    "\n",
    "Let's begin by setting up the necessary environment.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735371553/ogylhcgz3o8trjtnjcov.png\" alt=\"Gemini Reasoning Finetuning\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Get Access to Gemma 2\n",
    "\n",
    "To get access to Gemma 2, follow these steps:\n",
    "\n",
    "1. Sign in or register on [Kaggle](https://kaggle.com)\n",
    "\n",
    "2. Visit the [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma/frameworks/transformers) \n",
    "\n",
    "3. Accept the terms and conditions to gain access to the model\n",
    "\n",
    "4. Once approved, you can use the model in your Kaggle notebooks or download locally\n",
    "\n",
    "Note: The Gemma 2 family consists of different model sizes (2B and 7B parameters). For this tutorial, we'll be using the 2B parameter version to make the fine-tuning process more manageable on typical hardware configurations.\n",
    "\n",
    "\n",
    "Then\n",
    "`pip install kagglehub`\n",
    "\n",
    "```\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-2b\")\n",
    "\n",
    "print(\"Path to model files:\", path)\n",
    "\n",
    "```\n",
    "\n",
    "Copy the `path` and replace it at the 3) [Configuration `path`](#3-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U wandb nltk thefuzz python-Levenshtein bert-score evaluate transformers peft datasets janome numpy fuzzywuzzy bitsandbytes ml_dtypes tf_keras torch torchvision pytorch-lightning tensorflow scikit-learn tokenizers==0.20.1 huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/.cache/kagglehub/models/google/gemma-2/transformers/gemma-2-2b/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # Core Learning Parameters\n",
    "    \"learning_rate\": 5e-5,                  # How fast the model learns (0.00005)\n",
    "    \"continuous_thoughts\": 4,               # Number of latent space reasoning steps\n",
    "    \"stages\": 4,                            # Number of training curriculum stages\n",
    "    \"training_thoughts_sequence_length\": 50, # Number of thought sequence to generate\n",
    "\n",
    "    # Inference and Evaluation Params       \n",
    "    \"fuzzy_matcher_threshold\": 80,          # Fuzzy matcher threshold at 80%\n",
    "    \"cot_decoding_k\": 5,                    # Number of paths to try before finding the best answer\n",
    "\n",
    "    # Model Setup\n",
    "    \"max_length\": 256,                      # Maximum text length to process\n",
    "    \"model_name\": path,                     # Path to Gemma model\n",
    "    \"batch_size\": 4,                        # Number of examples processed together\n",
    "    \"weight_decay\": 0.01,                   # Helps prevent overfitting\n",
    "\n",
    "    # Special Tokens\n",
    "    \"bot_id\": \"<bot>\",                      # Marks start of latent reasoning\n",
    "    \"eot_id\": \"<eot>\",                      # Marks end of latent reasoning\n",
    "    \"answer_id\": \"<answer>\",                # Marks the begining of answer\n",
    "    \"debug\": True,                          # Enables debugging output. Also allows you see the model's thoughts\n",
    "\n",
    "    # Training Optimizations\n",
    "    \"bf16\": True,                           # Uses BFloat16 for faster training\n",
    "    \"per_device_train_batch_size\": 1,       # Samples per GPU/CPU\n",
    "    \"optim\": \"adamw_torch\",                 # AdamW optimizer for efficiency\n",
    "    \"wandb_project\": \"gemma2-finetuning\",   # Tracks training on Weights & Biases\n",
    "    \"logging_steps\": 1,                     # How often to log training progress\n",
    "    \"bf16_full_eval\": True,                 # Uses BFloat16 for evaluation\n",
    "    \"gradient_accumulation_steps\": 4,       # How often to update weights\n",
    "    \"save_steps\": 10000,                    # How often to save model\n",
    "    \"warmup_steps\": 0.1,                    # Number of warmup steps\n",
    "    \"output_dir\": \"output\",                 # Where to save model files\n",
    "    \"diversity_weight\": 0.1,                # Reasoning diversity weight\n",
    "    \"coherence_weight\": 0.1                 # Reasoning coherence weight\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Dataset Overview: Japanese-English Translation/Communication\n",
    "<i>Using the llm-japanese-dataset created by [Hirano et al. (2023)](https://arxiv.org/pdf/2305.12720)</i>\n",
    "\n",
    "We'll be using the llm-japanese-dataset (8.4M records) to fine-tune Gemma 2, focusing on Japanese-English translation and communication. The dataset follows this format:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Please translate to English.\n",
    "\n",
    "### Input:\n",
    "こんにちは、元気ですか？\n",
    "\n",
    "### Response:\n",
    "Hello, how are you?\n",
    "```\n",
    "\n",
    "Most of the data (about 80%) consists of translations like this, making it perfect for improving Gemma 2's Japanese-English capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:09.715221Z",
     "iopub.status.busy": "2024-12-16T18:12:09.714341Z",
     "iopub.status.idle": "2024-12-16T18:13:04.760441Z",
     "shell.execute_reply": "2024-12-16T18:13:04.759432Z",
     "shell.execute_reply.started": "2024-12-16T18:12:09.715185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from datasets import config as dataset_config\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# In your Python code\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/cache/huggingface'\n",
    "\n",
    "# Set HuggingFace datasets cache directory\n",
    "dataset_config.HF_DATASETS_CACHE = '/root/autodl-tmp/cache/datasets'\n",
    "\n",
    "\n",
    "dataset_name = \"izumi-lab/llm-japanese-dataset\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# For this tutorial, let's take 50,000k samples from the dataset\n",
    "item = 30000\n",
    "\n",
    "truncated_dataset = DatasetDict({\n",
    "    split: dataset[split].select(range(item))\n",
    "    for split in dataset.keys()\n",
    "})\n",
    "\n",
    "\n",
    "dataset = truncated_dataset\n",
    "eval_dataset = dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see few examples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:04.790751Z",
     "iopub.status.busy": "2024-12-16T18:13:04.790424Z",
     "iopub.status.idle": "2024-12-16T18:13:04.805684Z",
     "shell.execute_reply": "2024-12-16T18:13:04.804873Z",
     "shell.execute_reply.started": "2024-12-16T18:13:04.790716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  26文字 \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "Instruction:  人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  骨川（滑川も正解） \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "Instruction:  格闘家ボブ・サップの出身国はどこでしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  アメリカ \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Instruction: \", dataset['train'][\"instruction\"][i], \"\\n\")\n",
    "    print(\"Input: \", dataset['train'][\"input\"][i], \"\\n\")\n",
    "    print(\"Output: \",dataset['train'][\"output\"][i], \"\\n\")\n",
    "    print(f\"{'='*200}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Language Detector\n",
    "\n",
    "When handling multilingual content, detecting languages accurately is crucial. Our language detector analyzes both the structure and composition of mixed-language text.\n",
    "\n",
    "For example, let's examine this input:\n",
    "```python\n",
    "\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\"\n",
    "```\n",
    "\n",
    "The detector identifies Japanese as the primary language while recognizing English phrases (abc, first, ABC) embedded within. It analyzes the distribution of scripts including Hiragana, Latin alphabet, and various symbols. This composition analysis helps determine language percentages and structure.\n",
    "\n",
    "Beyond basic detection, this analysis guides translation strategy and validates output language alignment. By understanding the input language composition, Gemma 2 can better reason about how to process and generate appropriate bilingual responses. The detector becomes especially valuable when building chain-of-thought reasoning patterns across languages.\n",
    "\n",
    "Let's implement this detector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:15:41.502874Z",
     "iopub.status.busy": "2024-12-16T18:15:41.502091Z",
     "iopub.status.idle": "2024-12-16T18:15:41.517010Z",
     "shell.execute_reply": "2024-12-16T18:15:41.515979Z",
     "shell.execute_reply.started": "2024-12-16T18:15:41.502815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: スナフキン ===>>>> {'Japanese': 100.0}\n",
      "Text: レベッカ(REBECCA) ===>>>> {'English': 63.6, 'Japanese': 36.4}\n",
      "Text: Hello World ===>>>> {'English': 100.0}\n",
      "Text: こんにちは World! ===>>>> {'Japanese': 50.0, 'English': 50.0}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ScriptRange:\n",
    "    \"\"\"Represents a Unicode range for a writing system\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    name: str\n",
    "    \n",
    "class LanguageDetector:\n",
    "    def __init__(self):\n",
    "        self.scripts: List[ScriptRange] = []\n",
    "        self.language_mappings: Dict[str, List[str]] = {}\n",
    "        \n",
    "    def add_script(self, name: str, start: int, end: int) -> None:\n",
    "        \"\"\"\n",
    "        Add a new script range to the detector\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the script (e.g., 'Hiragana', 'Latin')\n",
    "            start: Starting Unicode code point\n",
    "            end: Ending Unicode code point\n",
    "        \"\"\"\n",
    "        self.scripts.append(ScriptRange(start, end, name))\n",
    "    \n",
    "    def map_scripts_to_language(self, language: str, script_names: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Map multiple scripts to a single language\n",
    "        \n",
    "        Args:\n",
    "            language: Name of the language (e.g., 'Japanese')\n",
    "            script_names: List of script names that belong to this language\n",
    "        \"\"\"\n",
    "        self.language_mappings[language] = script_names\n",
    "    \n",
    "    def detect(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Detect the percentage of different languages/scripts in the text\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping language/script names to their percentage presence\n",
    "        \"\"\"\n",
    "        # Count characters in each script\n",
    "        char_counts: Dict[str, int] = {script.name: 0 for script in self.scripts}\n",
    "        total_chars = 0\n",
    "        \n",
    "        for char in text:\n",
    "            if char.isspace() or char in '.,!?()[]{}':\n",
    "                continue\n",
    "                \n",
    "            code = ord(char)\n",
    "            total_chars += 1\n",
    "            \n",
    "            # Check which script range the character falls into\n",
    "            for script in self.scripts:\n",
    "                if script.start <= code <= script.end:\n",
    "                    char_counts[script.name] += 1\n",
    "                    break\n",
    "        \n",
    "        if total_chars == 0:\n",
    "            return {}\n",
    "            \n",
    "        # Calculate initial percentages\n",
    "        percentages = {\n",
    "            script: (count / total_chars) * 100\n",
    "            for script, count in char_counts.items()\n",
    "            if count > 0\n",
    "        }\n",
    "        \n",
    "        # Combine scripts into languages where applicable\n",
    "        final_percentages = {}\n",
    "        used_scripts = set()\n",
    "        \n",
    "        # First, handle mapped languages\n",
    "        for language, script_names in self.language_mappings.items():\n",
    "            total = sum(percentages.get(script, 0) for script in script_names)\n",
    "            if total > 0:\n",
    "                final_percentages[language] = total\n",
    "                used_scripts.update(script_names)\n",
    "        \n",
    "        # Then add remaining unmapped scripts\n",
    "        for script, percentage in percentages.items():\n",
    "            if script not in used_scripts:\n",
    "                final_percentages[script] = percentage\n",
    "        \n",
    "        return {k: round(v, 1) for k, v in sorted(\n",
    "            final_percentages.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )}\n",
    "\n",
    "# Example setup and usage\n",
    "def create_default_detector() -> LanguageDetector:\n",
    "    \"\"\"Create a detector with Japanese and English support\"\"\"\n",
    "    detector = LanguageDetector()\n",
    "    \n",
    "    # Add Japanese scripts\n",
    "    detector.add_script('Hiragana', 0x3040, 0x309F)\n",
    "    detector.add_script('Katakana', 0x30A0, 0x30FF)\n",
    "    detector.add_script('Kanji', 0x4E00, 0x9FFF)\n",
    "\n",
    "    # Add English scripts\n",
    "    detector.add_script('Latin', 0x0000, 0x024F)\n",
    "\n",
    "    \n",
    "    # Map scripts to languages\n",
    "    detector.map_scripts_to_language('Japanese', ['Hiragana', 'Katakana', 'Kanji'])\n",
    "    detector.map_scripts_to_language('English', ['Latin'])\n",
    "    \n",
    "    return detector\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = create_default_detector()\n",
    "    \n",
    "    test_texts = [\n",
    "        'スナフキン',\n",
    "        'レベッカ(REBECCA)',\n",
    "        'Hello World',\n",
    "        'こんにちは World!'\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        result = detector.detect(text)\n",
    "        print(f\"Text: {text} ===>>>> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Dataset Preprocessing\n",
    "\n",
    "The preprocessing pipeline prepares our dataset for Continuous Latent Reasoning by transforming raw translation pairs into structured training data. Here's what each component does:\n",
    "\n",
    "`preprocess_function`: \n",
    "Takes raw examples and generates Chain-of-Thought reasoning steps. For each sample, it:\n",
    "1. Analyzes language composition of input/output\n",
    "2. Generates appropriate reasoning steps in detected language\n",
    "3. Formats with special tokens (bot_token, eot_token) as per Hao et al. (2024)\n",
    "\n",
    "Example flow:\n",
    "```python\n",
    "Input: \"「abc ～the first～」へようこそ！\"\n",
    "Steps:\n",
    "- Detect languages (Japanese: 60%, English: 40%)\n",
    "- Generate understanding steps\n",
    "- Format with special tokens\n",
    "Output: \"<bos> Input <eos><bot><eot> Step 1... Step 2... Answer <eos>\"\n",
    "```\n",
    "\n",
    "`tokenizer_function`:\n",
    "Converts text into model inputs by:\n",
    "1. Tokenizing the formatted text\n",
    "2. Creating attention masks\n",
    "3. Preparing labels (masking question/thought tokens)\n",
    "\n",
    "\n",
    "This preprocessing ensures our data is properly structured for training Gemma 2 in continuous latent space reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def preprocess_function(\n",
    "    examples, \n",
    "    detector=None,  # Make detector optional\n",
    "    stages=1, \n",
    "    eos_token=\"<eos>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    language_config=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the input examples by constructing the prompt with reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input examples with keys \"instruction\", \"input\", and \"output\".\n",
    "        detector: A language detection object or function that detects the language of a given text.\n",
    "        stages (int): The number of reasoning stages to include in the prompt.\n",
    "        eos_token (str): The end-of-sequence token.\n",
    "        bos_token (str): The beginning-of-sequence token.\n",
    "        language_config (dict): A dictionary mapping language keys to their respective translations for steps and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    if language_config is None:\n",
    "        language_config = {\n",
    "            \"English\": {\n",
    "                \"language_detection\": \"Question language detection\",\n",
    "                \"understand_question\": \"Understand the question\",\n",
    "                \"understand_answer\": \"Understand the answer\",\n",
    "                \"response_language_detection\": \"Response language detection\",\n",
    "                \"answer_label\": \"Answer:\",\n",
    "                \"step_label\": \"Step\",\n",
    "            },\n",
    "            \"Japanese\": {\n",
    "                \"language_detection\": \"言語の検出\",\n",
    "                \"understand_question\": \"質問を理解する\",\n",
    "                \"understand_answer\": \"答えを理解する\",\n",
    "                \"response_language_detection\": \"応答言語の検出\",\n",
    "                \"answer_label\": \"答え：\",\n",
    "                \"step_label\": \"ステップ\",\n",
    "            },\n",
    "            # Add more languages here as needed\n",
    "        }\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    bot = config[\"bot_id\"]\n",
    "    eot = config[\"eot_id\"]\n",
    "    answer_token = config[\"answer_id\"]\n",
    "\n",
    "    # Initialize output dictionaries with lists\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(instructions)):\n",
    "        instruction = instructions[i]\n",
    "        input = inputs[i]\n",
    "        output = outputs[i]\n",
    "\n",
    "        if len(input) > 1:\n",
    "            input = instruction + input\n",
    "        else:\n",
    "            input = instruction\n",
    "\n",
    "        # Use the provided detector to detect languages\n",
    "        input_language = detector.detect(input) if detector else {\"English\": 100.0}  # Default to English if no detector\n",
    "        output_language = detector.detect(output) if detector else {\"English\": 100.0}  # Default to English if no detector\n",
    "\n",
    "        steps = []\n",
    "\n",
    "        # Determine the primary input and output languages\n",
    "        # Use the language key from the detector's output that matches a key in language_config\n",
    "        input_lang = next((lang for lang in input_language if lang in language_config), \"English\")\n",
    "        output_lang = next((lang for lang in output_language if lang in language_config), \"English\")\n",
    "\n",
    "        # Get the language-specific labels\n",
    "        input_labels = language_config.get(input_lang, language_config[\"English\"])\n",
    "        output_labels = language_config.get(output_lang, language_config[\"English\"])\n",
    "\n",
    "        # Input language detection\n",
    "        input_lang_str = \", \".join([f\"{k}: {v}%\" for k, v in input_language.items()])\n",
    "        steps.append(f\"{input_labels['language_detection']}: {input_lang_str}\")\n",
    "        steps.append(f\"{input_labels['understand_question']}: {input}\")\n",
    "        steps.append(f\"{input_labels['understand_answer']}: {output}\")\n",
    "\n",
    "        # Output language detection\n",
    "        output_lang_str = \", \".join([f\"{k}: {v}%\" for k, v in output_language.items()]) if output_language else \"Unknown\"\n",
    "        steps.append(f\"{output_labels['response_language_detection']}: {output_lang_str}\")\n",
    "\n",
    "        # Format steps with step numbers\n",
    "        steps = [f\"{output_labels['step_label']} {i+1} : {step}\" for i, step in enumerate(steps)]\n",
    "\n",
    "        # Include only the steps relevant to the current stage\n",
    "        if stages > 0:\n",
    "            steps = steps[-stages:]  # Keep the last `stages` steps\n",
    "\n",
    "        # Renumber steps to start from 1\n",
    "        steps = [f\"{output_labels['step_label']} {i+1} : {step.split(' : ')[1]}\" for i, step in enumerate(steps)]\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = bos_token + \"\\n\" + input + eos_token + bot + eot + \"\\n\" + \"\\n\".join(steps) + \"\\n\" + answer_token + output_labels['answer_label'] + output + eos_token\n",
    "\n",
    "        result.append(prompt)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenizer_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize the input prompt and prepare the input_ids, attention_mask, and labels for training.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input prompts.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = examples[\"prompt\"]\n",
    "    eot = config[\"eot_id\"]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        max_length=config[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    batch_size = labels.shape[0]\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(eot)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Find the positions of <eot> in the input_ids\n",
    "        eot_pos = (input_ids[i] == eot_id).nonzero(as_tuple=True)\n",
    "\n",
    "        if len(eot_pos[0]) > 0:\n",
    "            # Get the last occurrence of <eot>\n",
    "            last_eot_pos = eot_pos[0][-1].item()\n",
    "            \n",
    "            # Mask everything before and including the last <eot>\n",
    "            labels[i, :last_eot_pos] = -100\n",
    "\n",
    "        # Mask padding\n",
    "        labels[i, attention_mask[i] == 0] = -100\n",
    "\n",
    "\n",
    "    value =  {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        return value\n",
    "    else:\n",
    "        value[\"labels\"] = labels\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Config. If you need to handle more languages, you can add to this configuration and pick a suitable language detector as `LanguageDetector` class only support `English` and `Japanese` ... Note the keys `English`, `Japanese` or any other you intend to include must match the response from your Language detector `Text: レベッカ(REBECCA) ===>>>> {'English': 63.6, 'Japanese': 36.4}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_config = {\n",
    "    \"English\": {\n",
    "        \"language_detection\": \"Question language detection\",\n",
    "        \"understand_question\": \"Understand the question\",\n",
    "        \"understand_answer\": \"Understand the answer\",\n",
    "        \"response_language_detection\": \"Response language detection\",\n",
    "        \"answer_label\": \"Answer:\",\n",
    "        \"step_label\": \"Step\",\n",
    "    },\n",
    "    \"Japanese\": {\n",
    "        \"language_detection\": \"言語の検出\",\n",
    "        \"understand_question\": \"質問を理解する\",\n",
    "        \"understand_answer\": \"答えを理解する\",\n",
    "        \"response_language_detection\": \"応答言語の検出\",\n",
    "        \"answer_label\": \"答え：\",\n",
    "        \"step_label\": \"ステップ\",\n",
    "    },\n",
    "    # Add more languages here as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize what our dataset looks like preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: ========================>>>>>>>>>>>>>>>>> 0\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 59.3%, English: 25.9%\n",
      "ステップ 2 : 質問を理解する: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\n",
      "ステップ 3 : 答えを理解する: 26文字\n",
      "ステップ 4 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.1%\n",
      "ステップ 2 : 質問を理解する: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 4 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.2%\n",
      "ステップ 2 : 質問を理解する: 格闘家ボブ・サップの出身国はどこでしょう？\n",
      "ステップ 3 : 答えを理解する: アメリカ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.6%\n",
      "ステップ 2 : 質問を理解する: ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？\n",
      "ステップ 3 : 答えを理解する: クレムリン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.5%\n",
      "ステップ 2 : 質問を理解する: 織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？\n",
      "ステップ 3 : 答えを理解する: ホトトギス\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気タレント・タモリの本名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.0%\n",
      "ステップ 2 : 質問を理解する: 人気タレント・タモリの本名は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 森田一義\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：森田一義<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つのサイコロをふったとき、ゾロ目が出る確率は何分のいくつでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.2%\n",
      "ステップ 2 : 質問を理解する: ２つのサイコロをふったとき、ゾロ目が出る確率は何分のいくつでしょう？\n",
      "ステップ 3 : 答えを理解する: ６分の１\n",
      "ステップ 4 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：６分の１<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「国際連合」の旗に描かれている植物といったら何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.3%\n",
      "ステップ 2 : 質問を理解する: 「国際連合」の旗に描かれている植物といったら何でしょう？\n",
      "ステップ 3 : 答えを理解する: オリーブ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：オリーブ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アンデルセンの自叙伝とも言われている物語で、いじめられっこが実は白鳥だったというお話は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.9%\n",
      "ステップ 2 : 質問を理解する: アンデルセンの自叙伝とも言われている物語で、いじめられっこが実は白鳥だったというお話は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『みにくいあひるの子』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 81.8%\n",
      "<answer>答え：『みにくいあひるの子』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "男性の声の音域で、テノールとバスの間に当たるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.3%\n",
      "ステップ 2 : 質問を理解する: 男性の声の音域で、テノールとバスの間に当たるのは何でしょう？\n",
      "ステップ 3 : 答えを理解する: バリトン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：バリトン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "中華料理の飲茶で使われる、軽食やお菓子のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.8%\n",
      "ステップ 2 : 質問を理解する: 中華料理の飲茶で使われる、軽食やお菓子のことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 点心\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：点心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「たけやぶやけた」などのように、前から読んでも後ろから読んでも同じになる文章を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.7%\n",
      "ステップ 2 : 質問を理解する: 「たけやぶやけた」などのように、前から読んでも後ろから読んでも同じになる文章を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 回文\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：回文<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ゴルフボールの表面につけられているくぼみのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 97.0%\n",
      "ステップ 2 : 質問を理解する: ゴルフボールの表面につけられているくぼみのことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: ディンプル\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ディンプル<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "別名を『ラ・ジョコンダ』という、レオナルド・ダ・ヴィンチが描いた絵画といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.9%\n",
      "ステップ 2 : 質問を理解する: 別名を『ラ・ジョコンダ』という、レオナルド・ダ・ヴィンチが描いた絵画といえば何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『モナ・リザ』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 71.4%\n",
      "<answer>答え：『モナ・リザ』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「東京湾」は２つの半島に囲まれていますが、それは房総半島と何半島でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 86.5%\n",
      "ステップ 2 : 質問を理解する: 「東京湾」は２つの半島に囲まれていますが、それは房総半島と何半島でしょう？\n",
      "ステップ 3 : 答えを理解する: 三浦半島\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三浦半島<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "市販の自動車に手を加えて、性能を高めることを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.1%\n",
      "ステップ 2 : 質問を理解する: 市販の自動車に手を加えて、性能を高めることを英語で何というでしょう？\n",
      "ステップ 3 : 答えを理解する: チューンナップ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：チューンナップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1840年に勃発し、香港がイギリスに割譲される結果に終わった戦争を、原因となった麻薬から何というでしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 86.8%, English: 7.5%\n",
      "Step 2 : 質問を理解する: 1840年に勃発し、香港がイギリスに割譲される結果に終わった戦争を、原因となった麻薬から何というでしょう？\n",
      "Step 3 : 答えを理解する: アヘン戦争 (Opium War)\n",
      "Step 4 : Response language detection: English: 61.5%, Japanese: 38.5%\n",
      "<answer>Answer:アヘン戦争 (Opium War)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "気象用語で「平年」といえば、過去何年間の平均値のことでしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 87.1%\n",
      "Step 2 : 質問を理解する: 気象用語で「平年」といえば、過去何年間の平均値のことでしょう？\n",
      "Step 3 : 答えを理解する: 30年\n",
      "Step 4 : Response language detection: English: 66.7%, Japanese: 33.3%\n",
      "<answer>Answer:30年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1号は本郷猛、2号は一文字隼人がその正体である、テレビヒーローといえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.8%, English: 4.9%\n",
      "ステップ 2 : 質問を理解する: 1号は本郷猛、2号は一文字隼人がその正体である、テレビヒーローといえば何でしょう？\n",
      "ステップ 3 : 答えを理解する: 仮面ライダー\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：仮面ライダー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本国憲法に定められている国民の三大義務とは、勤労、教育と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.4%\n",
      "ステップ 2 : 質問を理解する: 日本国憲法に定められている国民の三大義務とは、勤労、教育と何でしょう？\n",
      "ステップ 3 : 答えを理解する: 納税\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：納税<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太棹、中棹、細棹といえば、どんな楽器の種類でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 84.6%\n",
      "ステップ 2 : 質問を理解する: 太棹、中棹、細棹といえば、どんな楽器の種類でしょう？\n",
      "ステップ 3 : 答えを理解する: 三味線\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三味線<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "代表作に『日本の黒い霧』や『点と線』などがある、推理作家は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 82.9%\n",
      "ステップ 2 : 質問を理解する: 代表作に『日本の黒い霧』や『点と線』などがある、推理作家は誰でしょう？\n",
      "ステップ 3 : 答えを理解する: 松本清張\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：松本清張<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "料理のときに使う計量スプーンで、大さじ１杯といったら通常何ccでしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 86.1%, English: 5.6%\n",
      "Step 2 : 質問を理解する: 料理のときに使う計量スプーンで、大さじ１杯といったら通常何ccでしょう？\n",
      "Step 3 : 答えを理解する: 15cc\n",
      "Step 4 : Response language detection: English: 100.0%\n",
      "<answer>Answer:15cc<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "スキー板に靴を取り付けるための締め具のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 96.8%\n",
      "ステップ 2 : 質問を理解する: スキー板に靴を取り付けるための締め具のことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: ビンディング\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ビンディング<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オーストラリアの国旗に星は６つありますが、ニュージーランドの国旗には星がいくつあるでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.5%\n",
      "ステップ 2 : 質問を理解する: オーストラリアの国旗に星は６つありますが、ニュージーランドの国旗には星がいくつあるでしょう？\n",
      "ステップ 3 : 答えを理解する: ４つ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：４つ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1934年には天然記念物に指定された、学名を「ニッポニア・ニッポン」という鳥といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 83.3%, English: 8.3%\n",
      "ステップ 2 : 質問を理解する: 1934年には天然記念物に指定された、学名を「ニッポニア・ニッポン」という鳥といえば何でしょう？\n",
      "ステップ 3 : 答えを理解する: トキ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：トキ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "火を神聖視するために「拝火教」の別名がある、紀元前６世紀のペルシアで始まった宗教は何教でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.6%\n",
      "ステップ 2 : 質問を理解する: 火を神聖視するために「拝火教」の別名がある、紀元前６世紀のペルシアで始まった宗教は何教でしょう？\n",
      "ステップ 3 : 答えを理解する: ゾロアスター教\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ゾロアスター教<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "テレビの視聴率調査における「ゴールデンタイム」とは、午後７時から何時までのことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.6%\n",
      "ステップ 2 : 質問を理解する: テレビの視聴率調査における「ゴールデンタイム」とは、午後７時から何時までのことでしょう？\n",
      "ステップ 3 : 答えを理解する: 午後１０時\n",
      "ステップ 4 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：午後１０時<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "他には影響を及ぼさない、狭い範囲でのもめ事のことを、「何の中の嵐」というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.8%\n",
      "ステップ 2 : 質問を理解する: 他には影響を及ぼさない、狭い範囲でのもめ事のことを、「何の中の嵐」というでしょう？\n",
      "ステップ 3 : 答えを理解する: コップ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "７０１年に藤原不比等（ふじわらのふひと）らが長部親王（おさべ）らと共に作り上げた、国家の基本法典を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 84.5%\n",
      "ステップ 2 : 質問を理解する: ７０１年に藤原不比等（ふじわらのふひと）らが長部親王（おさべ）らと共に作り上げた、国家の基本法典を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 大宝律令\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大宝律令<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "妊娠５ヶ月目の戌（いぬ）の日に体に付ける、胎児を保護するためにする帯を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.6%\n",
      "ステップ 2 : 質問を理解する: 妊娠５ヶ月目の戌（いぬ）の日に体に付ける、胎児を保護するためにする帯を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 岩田帯\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：岩田帯<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "セルバンテスの小説『ドン・キホーテ』で、ドン・キホーテが巨人と間違えて襲いかかったものは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 92.0%\n",
      "ステップ 2 : 質問を理解する: セルバンテスの小説『ドン・キホーテ』で、ドン・キホーテが巨人と間違えて襲いかかったものは何でしょう？\n",
      "ステップ 3 : 答えを理解する: 風車\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：風車<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ボウリングの球の重さを表す単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.5%\n",
      "ステップ 2 : 質問を理解する: ボウリングの球の重さを表す単位は何でしょう？\n",
      "ステップ 3 : 答えを理解する: ポンド\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポンド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "その色合いが鳩の血のように見えることから、最高級のルビーのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.1%\n",
      "ステップ 2 : 質問を理解する: その色合いが鳩の血のように見えることから、最高級のルビーのことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: ピジョンブラッド\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピジョンブラッド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "箱根や富士などの火山から噴き出した灰が積もって形成された、広く関東地方をおおっている赤い粘土質の地層を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 96.7%\n",
      "ステップ 2 : 質問を理解する: 箱根や富士などの火山から噴き出した灰が積もって形成された、広く関東地方をおおっている赤い粘土質の地層を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 関東ローム層\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：関東ローム層<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "地方から東京へ出ることを「上京」といいますが、地方から京都へ出ることを何といったでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.1%\n",
      "ステップ 2 : 質問を理解する: 地方から東京へ出ることを「上京」といいますが、地方から京都へ出ることを何といったでしょう？\n",
      "ステップ 3 : 答えを理解する: 上洛\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：上洛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本の国宝第1号に認定された弥勒菩薩像がある、京都の寺はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.1%, English: 2.9%\n",
      "ステップ 2 : 質問を理解する: 日本の国宝第1号に認定された弥勒菩薩像がある、京都の寺はどこでしょう?\n",
      "ステップ 3 : 答えを理解する: 広隆寺\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：広隆寺<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "牛２頭で１日に耕した面積が起源といわれる、ヤード・ポンド法の面積の単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.5%\n",
      "ステップ 2 : 質問を理解する: 牛２頭で１日に耕した面積が起源といわれる、ヤード・ポンド法の面積の単位は何でしょう？\n",
      "ステップ 3 : 答えを理解する: エーカー\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エーカー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『学校』シリーズや、『男はつらいよ』シリーズで知られる映画監督といえば誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 85.4%\n",
      "ステップ 2 : 質問を理解する: 『学校』シリーズや、『男はつらいよ』シリーズで知られる映画監督といえば誰でしょう？\n",
      "ステップ 3 : 答えを理解する: 山田洋次\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：山田洋次<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "いわゆる「四書五経」の「四書」とは、『大学』『孟子』『論語』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 67.6%\n",
      "ステップ 2 : 質問を理解する: いわゆる「四書五経」の「四書」とは、『大学』『孟子』『論語』と何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『中庸』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：『中庸』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "春の七草で「すずな」といえば蕪のことですが、「すずしろ」といえば何のことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 85.4%\n",
      "ステップ 2 : 質問を理解する: 春の七草で「すずな」といえば蕪のことですが、「すずしろ」といえば何のことでしょう？\n",
      "ステップ 3 : 答えを理解する: 大根\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大根<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年の元日に決勝が行われたサッカー天皇杯で、初優勝を飾ったチームは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.9%\n",
      "ステップ 2 : 質問を理解する: 今年の元日に決勝が行われたサッカー天皇杯で、初優勝を飾ったチームは何でしょう？\n",
      "ステップ 3 : 答えを理解する: 京都パープルサンガ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：京都パープルサンガ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ことわざ「弘法も筆の誤り」で、弘法大師が間違えたとされる文字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.2%\n",
      "ステップ 2 : 質問を理解する: ことわざ「弘法も筆の誤り」で、弘法大師が間違えたとされる文字は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 応\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：応<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "南米の国、ウルグアイとパラグアイは、共にどこの国の領土から独立したでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 92.1%\n",
      "ステップ 2 : 質問を理解する: 南米の国、ウルグアイとパラグアイは、共にどこの国の領土から独立したでしょう？\n",
      "ステップ 3 : 答えを理解する: スペイン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：スペイン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "北原白秋作詞の童謡『あわて床屋』の中で、床屋に行ったお客である動物は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.0%\n",
      "ステップ 2 : 質問を理解する: 北原白秋作詞の童謡『あわて床屋』の中で、床屋に行ったお客である動物は何でしょう？\n",
      "ステップ 3 : 答えを理解する: ウサギ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ウサギ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『Tokyo Walker』や『Kansai Walker』などの、「Walker」シリーズを発行している出版社といえばどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : Question language detection: English: 44.6%, Japanese: 43.1%\n",
      "ステップ 2 : Understand the question: 『Tokyo Walker』や『Kansai Walker』などの、「Walker」シリーズを発行している出版社といえばどこでしょう？\n",
      "ステップ 3 : Understand the answer: 角川書店\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：角川書店<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "50㎞以上250㎞以下の短い路線を運行する「地域航空輸送」を、英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 74.4%, English: 11.6%\n",
      "ステップ 2 : 質問を理解する: 50㎞以上250㎞以下の短い路線を運行する「地域航空輸送」を、英語で何というでしょう？\n",
      "ステップ 3 : 答えを理解する: コミューター航空\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コミューター航空<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロウソクの炎を3つに分けると外炎、内炎と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.5%, English: 3.8%\n",
      "ステップ 2 : 質問を理解する: ロウソクの炎を3つに分けると外炎、内炎と何でしょう？\n",
      "ステップ 3 : 答えを理解する: 炎心\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：炎心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つ以上の文字を組み合わせて図案化したもののことで、特にルイ・ヴィトンのものが知られるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.1%\n",
      "ステップ 2 : 質問を理解する: ２つ以上の文字を組み合わせて図案化したもののことで、特にルイ・ヴィトンのものが知られるのは何でしょう？\n",
      "ステップ 3 : 答えを理解する: モノグラム\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：モノグラム<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "東京の落語家で、真打ちと前座の中間にあるランクを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.9%\n",
      "ステップ 2 : 質問を理解する: 東京の落語家で、真打ちと前座の中間にあるランクを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 二ツ目\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：二ツ目<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大相撲の第６８代横綱・朝青龍は何部屋の力士でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.5%\n",
      "ステップ 2 : 質問を理解する: 大相撲の第６８代横綱・朝青龍は何部屋の力士でしょう？\n",
      "ステップ 3 : 答えを理解する: 高砂部屋\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高砂部屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ギリシャ神話で、ろうで固めた翼で空を飛んだものの、太陽の熱でろうが溶けて墜落死したのは誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.9%\n",
      "ステップ 2 : 質問を理解する: ギリシャ神話で、ろうで固めた翼で空を飛んだものの、太陽の熱でろうが溶けて墜落死したのは誰でしょう？\n",
      "ステップ 3 : 答えを理解する: イカロス\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イカロス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1924年以降、南アフリカの各地で発見され、ラテン語で「南の猿」という意味の名がつけられた化石人類は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 83.9%, English: 7.1%\n",
      "ステップ 2 : 質問を理解する: 1924年以降、南アフリカの各地で発見され、ラテン語で「南の猿」という意味の名がつけられた化石人類は何でしょう？\n",
      "ステップ 3 : 答えを理解する: アウストラロピテクス\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アウストラロピテクス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デモテープ、デモ行進のデモとは、どんな言葉を略したものでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.6%\n",
      "ステップ 2 : 質問を理解する: デモテープ、デモ行進のデモとは、どんな言葉を略したものでしょう？\n",
      "ステップ 3 : 答えを理解する: デモンストレーション\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：デモンストレーション<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "この時期を表した俳句「春の海 ひねもすのたり のたりかな」を詠んだ、江戸時代の俳人は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.3%\n",
      "ステップ 2 : 質問を理解する: この時期を表した俳句「春の海 ひねもすのたり のたりかな」を詠んだ、江戸時代の俳人は誰でしょう？\n",
      "ステップ 3 : 答えを理解する: 与謝蕪村\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与謝蕪村<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシアの作曲家、チャイコフスキーの三大バレエ音楽といえば、『白鳥の湖』『くるみ割り人形』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 86.3%\n",
      "ステップ 2 : 質問を理解する: ロシアの作曲家、チャイコフスキーの三大バレエ音楽といえば、『白鳥の湖』『くるみ割り人形』と何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『眠れる森の美女』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：『眠れる森の美女』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロイターといえばイギリスの通信社ですが、ＡＦＰといえばどこの国の通信社でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.5%\n",
      "ステップ 2 : 質問を理解する: ロイターといえばイギリスの通信社ですが、ＡＦＰといえばどこの国の通信社でしょう？\n",
      "ステップ 3 : 答えを理解する: フランス\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：フランス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ジャムやマーマレードを入れて飲む紅茶の飲み方を、ある国の名を取って何ティーというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.6%\n",
      "ステップ 2 : 質問を理解する: ジャムやマーマレードを入れて飲む紅茶の飲み方を、ある国の名を取って何ティーというでしょう？\n",
      "ステップ 3 : 答えを理解する: ロシアンティー\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロシアンティー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリンピックの五輪の旗で、真ん中にある輪の色は何色でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.3%\n",
      "ステップ 2 : 質問を理解する: オリンピックの五輪の旗で、真ん中にある輪の色は何色でしょう？\n",
      "ステップ 3 : 答えを理解する: 黒\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：黒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太陽系の惑星の中で衛星をもっていない2つの惑星とは、水星と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.4%, English: 2.9%\n",
      "ステップ 2 : 質問を理解する: 太陽系の惑星の中で衛星をもっていない2つの惑星とは、水星と何でしょう？\n",
      "ステップ 3 : 答えを理解する: 金星\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：金星<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＡＢＢＡのヒットソングに綴ってストーリーが展開される、電通四季劇場「海」で公開中の劇団四季のミュージカルは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 86.4%\n",
      "ステップ 2 : 質問を理解する: ＡＢＢＡのヒットソングに綴ってストーリーが展開される、電通四季劇場「海」で公開中の劇団四季のミュージカルは何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『マンマ・ミーア！』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 70.0%\n",
      "<answer>答え：『マンマ・ミーア！』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "エレベーターで使われる記号で、屋上を示す「Ｒ」はルーフの略ですが、地下を示す「Ｂ」はどんな単語の頭文字でしょう？（カタカナ可）<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 82.5%\n",
      "ステップ 2 : 質問を理解する: エレベーターで使われる記号で、屋上を示す「Ｒ」はルーフの略ですが、地下を示す「Ｂ」はどんな単語の頭文字でしょう？（カタカナ可）\n",
      "ステップ 3 : 答えを理解する: ベースメント\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ベースメント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インドではイギリスが東インド会社を設立し、日本では関ケ原の戦いの火ぶたが切って落とされたのは、西暦何年のことでしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 94.9%\n",
      "Step 2 : 質問を理解する: インドではイギリスが東インド会社を設立し、日本では関ケ原の戦いの火ぶたが切って落とされたのは、西暦何年のことでしょう？\n",
      "Step 3 : 答えを理解する: 1600年\n",
      "Step 4 : Response language detection: English: 80.0%, Japanese: 20.0%\n",
      "<answer>Answer:1600年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "とっくりを持ったタヌキの置物が有名な、滋賀県の焼物といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.3%\n",
      "ステップ 2 : 質問を理解する: とっくりを持ったタヌキの置物が有名な、滋賀県の焼物といえば何でしょう？\n",
      "ステップ 3 : 答えを理解する: 信楽（しがらき）焼\n",
      "ステップ 4 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：信楽（しがらき）焼<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本では上松美香（あげまつみか）がその演奏者として知られている、「インディアンハープ」とも言われる民族楽器は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.0%\n",
      "ステップ 2 : 質問を理解する: 日本では上松美香（あげまつみか）がその演奏者として知られている、「インディアンハープ」とも言われる民族楽器は何でしょう？\n",
      "ステップ 3 : 答えを理解する: アルパ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アルパ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカのブッシュ (George Walker Bush) 現大統領が昨年の一般教書演説で使って以来広まった、イラン・北朝鮮・イラクの批判的な総称は何でしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 76.0%, English: 21.3%\n",
      "Step 2 : 質問を理解する: アメリカのブッシュ (George Walker Bush) 現大統領が昨年の一般教書演説で使って以来広まった、イラン・北朝鮮・イラクの批判的な総称は何でしょう？\n",
      "Step 3 : 答えを理解する: 悪の枢軸 (Axis of Evil)\n",
      "Step 4 : Response language detection: English: 71.4%, Japanese: 28.6%\n",
      "<answer>Answer:悪の枢軸 (Axis of Evil)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "木下順二の戯曲『夕鶴』で、助けた鶴の化身・つうと結婚する男の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.7%\n",
      "ステップ 2 : 質問を理解する: 木下順二の戯曲『夕鶴』で、助けた鶴の化身・つうと結婚する男の名前は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 与ひょう\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与ひょう<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "幾つかの布切れを縫いあわせ、飾りや模様を作る手芸方法を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.4%\n",
      "ステップ 2 : 質問を理解する: 幾つかの布切れを縫いあわせ、飾りや模様を作る手芸方法を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: パッチワーク\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：パッチワーク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "紫外線ＵＶ－Ｂの作用によって起こる、皮膚が褐色になる健康的な日焼けを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.0%\n",
      "ステップ 2 : 質問を理解する: 紫外線ＵＶ－Ｂの作用によって起こる、皮膚が褐色になる健康的な日焼けを英語で何というでしょう？\n",
      "ステップ 3 : 答えを理解する: サンタン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：サンタン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカ合衆国で、一番面積が広い州はアラスカ州ですが、一番面積が狭い州はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.2%\n",
      "ステップ 2 : 質問を理解する: アメリカ合衆国で、一番面積が広い州はアラスカ州ですが、一番面積が狭い州はどこでしょう?\n",
      "ステップ 3 : 答えを理解する: ロードアイランド州\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロードアイランド州<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フェンシングの試合場のことを何というでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 100.0%\n",
      "ステップ 2 : 質問を理解する: フェンシングの試合場のことを何というでしょう?\n",
      "ステップ 3 : 答えを理解する: ピスト\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主著に『純粋理性批判』『実践理性批判』がある、ドイツ観念論の創始者である哲学者といえば誰でしょう?<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.6%\n",
      "ステップ 2 : 質問を理解する: 主著に『純粋理性批判』『実践理性批判』がある、ドイツ観念論の創始者である哲学者といえば誰でしょう?\n",
      "ステップ 3 : 答えを理解する: イマニュエル・カント\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イマニュエル・カント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "態度や意見ががらりと変わってしまうことを、ある動物の斑紋が季節によって鮮やかに変わることから何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 96.4%\n",
      "ステップ 2 : 質問を理解する: 態度や意見ががらりと変わってしまうことを、ある動物の斑紋が季節によって鮮やかに変わることから何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 豹変\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：豹変<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "英語の『ドレミの歌』の歌詞で、「自分を呼ぶ名前」と歌われている音は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 84.6%\n",
      "ステップ 2 : 質問を理解する: 英語の『ドレミの歌』の歌詞で、「自分を呼ぶ名前」と歌われている音は何でしょう？\n",
      "ステップ 3 : 答えを理解する: ミ（Ｍｅ、Ｍｉ）\n",
      "ステップ 4 : 応答言語の検出: Japanese: 12.5%\n",
      "<answer>答え：ミ（Ｍｅ、Ｍｉ）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年秋に開幕する、ラグビーの社会人１２チームによる全国規模のリーグ戦を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.9%\n",
      "ステップ 2 : 質問を理解する: 今年秋に開幕する、ラグビーの社会人１２チームによる全国規模のリーグ戦を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: （ジャパンラグビー）トップリーグ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 87.5%\n",
      "<answer>答え：（ジャパンラグビー）トップリーグ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イスラム文化で用いられた、植物の茎や葉を組み合わせて作った幾何学文様や唐草文様を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.9%\n",
      "ステップ 2 : 質問を理解する: イスラム文化で用いられた、植物の茎や葉を組み合わせて作った幾何学文様や唐草文様を何というでしょう？\n",
      "ステップ 3 : 答えを理解する: アラベスク\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アラベスク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「郵政三事業の民営化」などというときの「三事業」とは、「郵便」「貯金」とあと１つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 76.6%\n",
      "ステップ 2 : 質問を理解する: 「郵政三事業の民営化」などというときの「三事業」とは、「郵便」「貯金」とあと１つは何でしょう？\n",
      "ステップ 3 : 答えを理解する: 簡易保険（保険は×）\n",
      "ステップ 4 : 応答言語の検出: Japanese: 70.0%, English: 10.0%\n",
      "<answer>答え：簡易保険（保険は×）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主に夏の夜に行われる､まきを焚いて野外で行う能のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.4%\n",
      "ステップ 2 : 質問を理解する: 主に夏の夜に行われる､まきを焚いて野外で行う能のことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 薪能(たきぎのう)\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：薪能(たきぎのう)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大きな火皿を持ち、吸い口までが湾曲したパイプを、船乗りがよく使ったことから何パイプというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.9%\n",
      "ステップ 2 : 質問を理解する: 大きな火皿を持ち、吸い口までが湾曲したパイプを、船乗りがよく使ったことから何パイプというでしょう？\n",
      "ステップ 3 : 答えを理解する: マドロスパイプ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：マドロスパイプ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "青銅といえば銅と錫の合金ですが、黄銅は銅と何との合金でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.5%\n",
      "ステップ 2 : 質問を理解する: 青銅といえば銅と錫の合金ですが、黄銅は銅と何との合金でしょう？\n",
      "ステップ 3 : 答えを理解する: 亜鉛\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：亜鉛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "江戸時代の村役人で「村方三役」と呼ばれたのは、庄屋、組頭とあと一つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.5%\n",
      "ステップ 2 : 質問を理解する: 江戸時代の村役人で「村方三役」と呼ばれたのは、庄屋、組頭とあと一つは何でしょう？\n",
      "ステップ 3 : 答えを理解する: 百姓代\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：百姓代<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ハクチョウを県の鳥にしている２つの県といえば、青森県とどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.2%\n",
      "ステップ 2 : 質問を理解する: ハクチョウを県の鳥にしている２つの県といえば、青森県とどこでしょう？\n",
      "ステップ 3 : 答えを理解する: 島根県\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：島根県<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ずばり、卓球は1セット何点先取でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 85.0%, English: 5.0%\n",
      "ステップ 2 : 質問を理解する: ずばり、卓球は1セット何点先取でしょう？\n",
      "ステップ 3 : 答えを理解する: 11点（昨年よりルール改正）\n",
      "ステップ 4 : 応答言語の検出: Japanese: 71.4%, English: 14.3%\n",
      "<answer>答え：11点（昨年よりルール改正）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＪＩＳ規格のキーボードでのタッチタイピングで、人差し指を置く基本となるキーは「Ｆ」と何でしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 83.3%\n",
      "Step 2 : 質問を理解する: ＪＩＳ規格のキーボードでのタッチタイピングで、人差し指を置く基本となるキーは「Ｆ」と何でしょう？\n",
      "Step 3 : 答えを理解する: Ｊ\n",
      "Step 4 : Response language detection: Unknown\n",
      "<answer>Answer:Ｊ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "映画『ロッキー』で、女優タリア・シャイアが演じた、ロッキーの妻の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.8%\n",
      "ステップ 2 : 質問を理解する: 映画『ロッキー』で、女優タリア・シャイアが演じた、ロッキーの妻の名前は何でしょう？\n",
      "ステップ 3 : 答えを理解する: エイドリアン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エイドリアン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インクのしみを落として作った左右対称の図形が何に見えるかで診断する人格検査法を、スイスの精神医学者の名前を取って何テストというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 97.1%\n",
      "ステップ 2 : 質問を理解する: インクのしみを落として作った左右対称の図形が何に見えるかで診断する人格検査法を、スイスの精神医学者の名前を取って何テストというでしょう？\n",
      "ステップ 3 : 答えを理解する: ロールシャッハ・テスト\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロールシャッハ・テスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「行く河の流れは絶えずして、しかも元の水にあらず」という書き出しの、鴨長明の随筆といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.0%\n",
      "ステップ 2 : 質問を理解する: 「行く河の流れは絶えずして、しかも元の水にあらず」という書き出しの、鴨長明の随筆といえば何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『方丈記』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：『方丈記』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "嵐、アルフィー、ミスターチルドレンの３つのグループのメンバーに共通する名字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.9%\n",
      "ステップ 2 : 質問を理解する: 嵐、アルフィー、ミスターチルドレンの３つのグループのメンバーに共通する名字は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 桜井\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：桜井<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "立秋の後の暑さを「残暑」といいますが、立春の後の寒さのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.7%\n",
      "ステップ 2 : 質問を理解する: 立秋の後の暑さを「残暑」といいますが、立春の後の寒さのことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: 余寒\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：余寒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "出世魚ボラの成長しきった時の名に因んだ、結局のところ、という意味の言葉は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 92.9%\n",
      "ステップ 2 : 質問を理解する: 出世魚ボラの成長しきった時の名に因んだ、結局のところ、という意味の言葉は何でしょう？\n",
      "ステップ 3 : 答えを理解する: とどのつまり\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：とどのつまり<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "カジノのルーレットにある数字の中で、最大の数字はいくつでしょう？<eos><bot><eot>\n",
      "Step 1 : 言語の検出: Japanese: 93.8%\n",
      "Step 2 : 質問を理解する: カジノのルーレットにある数字の中で、最大の数字はいくつでしょう？\n",
      "Step 3 : 答えを理解する: 36\n",
      "Step 4 : Response language detection: English: 100.0%\n",
      "<answer>Answer:36<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フォークボールを武器に通算215勝を挙げ、｢フォークボールの神様｣の異名を取った中日の投手は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 86.5%, English: 5.8%\n",
      "ステップ 2 : 質問を理解する: フォークボールを武器に通算215勝を挙げ、｢フォークボールの神様｣の異名を取った中日の投手は誰でしょう？\n",
      "ステップ 3 : 答えを理解する: 杉下茂\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：杉下茂<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "チベットのラサにある、ダライ・ダマが住む宮殿といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.8%\n",
      "ステップ 2 : 質問を理解する: チベットのラサにある、ダライ・ダマが住む宮殿といえば何でしょう？\n",
      "ステップ 3 : 答えを理解する: ポタラ宮\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポタラ宮<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イギリスで中世の封建制度崩壊と共に現れた独立自営農民のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 97.4%\n",
      "ステップ 2 : 質問を理解する: イギリスで中世の封建制度崩壊と共に現れた独立自営農民のことを何というでしょう？\n",
      "ステップ 3 : 答えを理解する: ヨーマン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ヨーマン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "小木博明(おぎ・ひろあき）と矢作兼（やはぎ・けん）の２人が組んでいる、お笑いコンビの名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.0%\n",
      "ステップ 2 : 質問を理解する: 小木博明(おぎ・ひろあき）と矢作兼（やはぎ・けん）の２人が組んでいる、お笑いコンビの名前は何でしょう？\n",
      "ステップ 3 : 答えを理解する: おぎやはぎ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：おぎやはぎ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "果物を切る時などに使われる小型のナイフのことを、和製英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 94.7%\n",
      "ステップ 2 : 質問を理解する: 果物を切る時などに使われる小型のナイフのことを、和製英語で何というでしょう？\n",
      "ステップ 3 : 答えを理解する: ぺティナイフ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ぺティナイフ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "コーヒーはアカネ科の植物ですが、カカオは何科の植物でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 93.3%\n",
      "ステップ 2 : 質問を理解する: コーヒーはアカネ科の植物ですが、カカオは何科の植物でしょう？\n",
      "ステップ 3 : 答えを理解する: アオギリ科\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アオギリ科<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デパートの「友の会」で、「クローバーサークル」は伊勢丹ですが、「ローズサークル」といえばどこのデパートでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 83.9%\n",
      "ステップ 2 : 質問を理解する: デパートの「友の会」で、「クローバーサークル」は伊勢丹ですが、「ローズサークル」といえばどこのデパートでしょう？\n",
      "ステップ 3 : 答えを理解する: 高島屋\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高島屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリコンのシングルチャートで初めて「デビュー曲初登場1位」となった、近藤真彦のヒット曲は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 90.0%, English: 2.0%\n",
      "ステップ 2 : 質問を理解する: オリコンのシングルチャートで初めて「デビュー曲初登場1位」となった、近藤真彦のヒット曲は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 『スニーカーぶる～す』\n",
      "ステップ 4 : 応答言語の検出: Japanese: 72.7%\n",
      "<answer>答え：『スニーカーぶる～す』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "年に２度収穫できる「二度芋」といえばジャガイモのことですが、年に３度収穫できることから「三度豆」と呼ばれているのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 87.3%\n",
      "ステップ 2 : 質問を理解する: 年に２度収穫できる「二度芋」といえばジャガイモのことですが、年に３度収穫できることから「三度豆」と呼ばれているのは何でしょう？\n",
      "ステップ 3 : 答えを理解する: インゲン豆\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：インゲン豆<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Stage: ========================>>>>>>>>>>>>>>>>> 1\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気タレント・タモリの本名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：森田一義<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つのサイコロをふったとき、ゾロ目が出る確率は何分のいくつでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：６分の１<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「国際連合」の旗に描かれている植物といったら何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：オリーブ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アンデルセンの自叙伝とも言われている物語で、いじめられっこが実は白鳥だったというお話は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 81.8%\n",
      "<answer>答え：『みにくいあひるの子』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "男性の声の音域で、テノールとバスの間に当たるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：バリトン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "中華料理の飲茶で使われる、軽食やお菓子のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：点心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「たけやぶやけた」などのように、前から読んでも後ろから読んでも同じになる文章を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：回文<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ゴルフボールの表面につけられているくぼみのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ディンプル<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "別名を『ラ・ジョコンダ』という、レオナルド・ダ・ヴィンチが描いた絵画といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 71.4%\n",
      "<answer>答え：『モナ・リザ』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「東京湾」は２つの半島に囲まれていますが、それは房総半島と何半島でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三浦半島<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "市販の自動車に手を加えて、性能を高めることを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：チューンナップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1840年に勃発し、香港がイギリスに割譲される結果に終わった戦争を、原因となった麻薬から何というでしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: English: 61.5%, Japanese: 38.5%\n",
      "<answer>Answer:アヘン戦争 (Opium War)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "気象用語で「平年」といえば、過去何年間の平均値のことでしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: English: 66.7%, Japanese: 33.3%\n",
      "<answer>Answer:30年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1号は本郷猛、2号は一文字隼人がその正体である、テレビヒーローといえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：仮面ライダー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本国憲法に定められている国民の三大義務とは、勤労、教育と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：納税<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太棹、中棹、細棹といえば、どんな楽器の種類でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三味線<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "代表作に『日本の黒い霧』や『点と線』などがある、推理作家は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：松本清張<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "料理のときに使う計量スプーンで、大さじ１杯といったら通常何ccでしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: English: 100.0%\n",
      "<answer>Answer:15cc<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "スキー板に靴を取り付けるための締め具のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ビンディング<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オーストラリアの国旗に星は６つありますが、ニュージーランドの国旗には星がいくつあるでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：４つ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1934年には天然記念物に指定された、学名を「ニッポニア・ニッポン」という鳥といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：トキ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "火を神聖視するために「拝火教」の別名がある、紀元前６世紀のペルシアで始まった宗教は何教でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ゾロアスター教<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "テレビの視聴率調査における「ゴールデンタイム」とは、午後７時から何時までのことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：午後１０時<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "他には影響を及ぼさない、狭い範囲でのもめ事のことを、「何の中の嵐」というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "７０１年に藤原不比等（ふじわらのふひと）らが長部親王（おさべ）らと共に作り上げた、国家の基本法典を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大宝律令<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "妊娠５ヶ月目の戌（いぬ）の日に体に付ける、胎児を保護するためにする帯を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：岩田帯<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "セルバンテスの小説『ドン・キホーテ』で、ドン・キホーテが巨人と間違えて襲いかかったものは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：風車<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ボウリングの球の重さを表す単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポンド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "その色合いが鳩の血のように見えることから、最高級のルビーのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピジョンブラッド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "箱根や富士などの火山から噴き出した灰が積もって形成された、広く関東地方をおおっている赤い粘土質の地層を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：関東ローム層<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "地方から東京へ出ることを「上京」といいますが、地方から京都へ出ることを何といったでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：上洛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本の国宝第1号に認定された弥勒菩薩像がある、京都の寺はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：広隆寺<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "牛２頭で１日に耕した面積が起源といわれる、ヤード・ポンド法の面積の単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エーカー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『学校』シリーズや、『男はつらいよ』シリーズで知られる映画監督といえば誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：山田洋次<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "いわゆる「四書五経」の「四書」とは、『大学』『孟子』『論語』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：『中庸』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "春の七草で「すずな」といえば蕪のことですが、「すずしろ」といえば何のことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大根<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年の元日に決勝が行われたサッカー天皇杯で、初優勝を飾ったチームは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：京都パープルサンガ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ことわざ「弘法も筆の誤り」で、弘法大師が間違えたとされる文字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：応<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "南米の国、ウルグアイとパラグアイは、共にどこの国の領土から独立したでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：スペイン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "北原白秋作詞の童謡『あわて床屋』の中で、床屋に行ったお客である動物は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ウサギ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『Tokyo Walker』や『Kansai Walker』などの、「Walker」シリーズを発行している出版社といえばどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：角川書店<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "50㎞以上250㎞以下の短い路線を運行する「地域航空輸送」を、英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コミューター航空<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロウソクの炎を3つに分けると外炎、内炎と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：炎心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つ以上の文字を組み合わせて図案化したもののことで、特にルイ・ヴィトンのものが知られるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：モノグラム<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "東京の落語家で、真打ちと前座の中間にあるランクを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：二ツ目<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大相撲の第６８代横綱・朝青龍は何部屋の力士でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高砂部屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ギリシャ神話で、ろうで固めた翼で空を飛んだものの、太陽の熱でろうが溶けて墜落死したのは誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イカロス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1924年以降、南アフリカの各地で発見され、ラテン語で「南の猿」という意味の名がつけられた化石人類は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アウストラロピテクス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デモテープ、デモ行進のデモとは、どんな言葉を略したものでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：デモンストレーション<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "この時期を表した俳句「春の海 ひねもすのたり のたりかな」を詠んだ、江戸時代の俳人は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与謝蕪村<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシアの作曲家、チャイコフスキーの三大バレエ音楽といえば、『白鳥の湖』『くるみ割り人形』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：『眠れる森の美女』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロイターといえばイギリスの通信社ですが、ＡＦＰといえばどこの国の通信社でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：フランス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ジャムやマーマレードを入れて飲む紅茶の飲み方を、ある国の名を取って何ティーというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロシアンティー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリンピックの五輪の旗で、真ん中にある輪の色は何色でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：黒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太陽系の惑星の中で衛星をもっていない2つの惑星とは、水星と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：金星<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＡＢＢＡのヒットソングに綴ってストーリーが展開される、電通四季劇場「海」で公開中の劇団四季のミュージカルは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 70.0%\n",
      "<answer>答え：『マンマ・ミーア！』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "エレベーターで使われる記号で、屋上を示す「Ｒ」はルーフの略ですが、地下を示す「Ｂ」はどんな単語の頭文字でしょう？（カタカナ可）<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ベースメント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インドではイギリスが東インド会社を設立し、日本では関ケ原の戦いの火ぶたが切って落とされたのは、西暦何年のことでしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: English: 80.0%, Japanese: 20.0%\n",
      "<answer>Answer:1600年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "とっくりを持ったタヌキの置物が有名な、滋賀県の焼物といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：信楽（しがらき）焼<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本では上松美香（あげまつみか）がその演奏者として知られている、「インディアンハープ」とも言われる民族楽器は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アルパ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカのブッシュ (George Walker Bush) 現大統領が昨年の一般教書演説で使って以来広まった、イラン・北朝鮮・イラクの批判的な総称は何でしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: English: 71.4%, Japanese: 28.6%\n",
      "<answer>Answer:悪の枢軸 (Axis of Evil)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "木下順二の戯曲『夕鶴』で、助けた鶴の化身・つうと結婚する男の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与ひょう<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "幾つかの布切れを縫いあわせ、飾りや模様を作る手芸方法を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：パッチワーク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "紫外線ＵＶ－Ｂの作用によって起こる、皮膚が褐色になる健康的な日焼けを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：サンタン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカ合衆国で、一番面積が広い州はアラスカ州ですが、一番面積が狭い州はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロードアイランド州<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フェンシングの試合場のことを何というでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主著に『純粋理性批判』『実践理性批判』がある、ドイツ観念論の創始者である哲学者といえば誰でしょう?<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イマニュエル・カント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "態度や意見ががらりと変わってしまうことを、ある動物の斑紋が季節によって鮮やかに変わることから何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：豹変<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "英語の『ドレミの歌』の歌詞で、「自分を呼ぶ名前」と歌われている音は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 12.5%\n",
      "<answer>答え：ミ（Ｍｅ、Ｍｉ）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年秋に開幕する、ラグビーの社会人１２チームによる全国規模のリーグ戦を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 87.5%\n",
      "<answer>答え：（ジャパンラグビー）トップリーグ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イスラム文化で用いられた、植物の茎や葉を組み合わせて作った幾何学文様や唐草文様を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アラベスク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「郵政三事業の民営化」などというときの「三事業」とは、「郵便」「貯金」とあと１つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 70.0%, English: 10.0%\n",
      "<answer>答え：簡易保険（保険は×）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主に夏の夜に行われる､まきを焚いて野外で行う能のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：薪能(たきぎのう)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大きな火皿を持ち、吸い口までが湾曲したパイプを、船乗りがよく使ったことから何パイプというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：マドロスパイプ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "青銅といえば銅と錫の合金ですが、黄銅は銅と何との合金でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：亜鉛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "江戸時代の村役人で「村方三役」と呼ばれたのは、庄屋、組頭とあと一つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：百姓代<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ハクチョウを県の鳥にしている２つの県といえば、青森県とどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：島根県<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ずばり、卓球は1セット何点先取でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 71.4%, English: 14.3%\n",
      "<answer>答え：11点（昨年よりルール改正）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＪＩＳ規格のキーボードでのタッチタイピングで、人差し指を置く基本となるキーは「Ｆ」と何でしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: Unknown\n",
      "<answer>Answer:Ｊ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "映画『ロッキー』で、女優タリア・シャイアが演じた、ロッキーの妻の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エイドリアン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インクのしみを落として作った左右対称の図形が何に見えるかで診断する人格検査法を、スイスの精神医学者の名前を取って何テストというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロールシャッハ・テスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「行く河の流れは絶えずして、しかも元の水にあらず」という書き出しの、鴨長明の随筆といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：『方丈記』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "嵐、アルフィー、ミスターチルドレンの３つのグループのメンバーに共通する名字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：桜井<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "立秋の後の暑さを「残暑」といいますが、立春の後の寒さのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：余寒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "出世魚ボラの成長しきった時の名に因んだ、結局のところ、という意味の言葉は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：とどのつまり<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "カジノのルーレットにある数字の中で、最大の数字はいくつでしょう？<eos><bot><eot>\n",
      "Step 1 : Response language detection: English: 100.0%\n",
      "<answer>Answer:36<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フォークボールを武器に通算215勝を挙げ、｢フォークボールの神様｣の異名を取った中日の投手は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：杉下茂<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "チベットのラサにある、ダライ・ダマが住む宮殿といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポタラ宮<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イギリスで中世の封建制度崩壊と共に現れた独立自営農民のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ヨーマン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "小木博明(おぎ・ひろあき）と矢作兼（やはぎ・けん）の２人が組んでいる、お笑いコンビの名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：おぎやはぎ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "果物を切る時などに使われる小型のナイフのことを、和製英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ぺティナイフ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "コーヒーはアカネ科の植物ですが、カカオは何科の植物でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アオギリ科<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デパートの「友の会」で、「クローバーサークル」は伊勢丹ですが、「ローズサークル」といえばどこのデパートでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高島屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリコンのシングルチャートで初めて「デビュー曲初登場1位」となった、近藤真彦のヒット曲は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 72.7%\n",
      "<answer>答え：『スニーカーぶる～す』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "年に２度収穫できる「二度芋」といえばジャガイモのことですが、年に３度収穫できることから「三度豆」と呼ばれているのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：インゲン豆<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Stage: ========================>>>>>>>>>>>>>>>>> 2\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 26文字\n",
      "ステップ 2 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 2 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アメリカ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: クレムリン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ホトトギス\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気タレント・タモリの本名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 森田一義\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：森田一義<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つのサイコロをふったとき、ゾロ目が出る確率は何分のいくつでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ６分の１\n",
      "ステップ 2 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：６分の１<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「国際連合」の旗に描かれている植物といったら何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: オリーブ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：オリーブ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アンデルセンの自叙伝とも言われている物語で、いじめられっこが実は白鳥だったというお話は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『みにくいあひるの子』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 81.8%\n",
      "<answer>答え：『みにくいあひるの子』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "男性の声の音域で、テノールとバスの間に当たるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: バリトン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：バリトン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "中華料理の飲茶で使われる、軽食やお菓子のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 点心\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：点心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「たけやぶやけた」などのように、前から読んでも後ろから読んでも同じになる文章を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 回文\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：回文<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ゴルフボールの表面につけられているくぼみのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ディンプル\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ディンプル<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "別名を『ラ・ジョコンダ』という、レオナルド・ダ・ヴィンチが描いた絵画といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『モナ・リザ』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 71.4%\n",
      "<answer>答え：『モナ・リザ』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「東京湾」は２つの半島に囲まれていますが、それは房総半島と何半島でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 三浦半島\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三浦半島<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "市販の自動車に手を加えて、性能を高めることを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: チューンナップ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：チューンナップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1840年に勃発し、香港がイギリスに割譲される結果に終わった戦争を、原因となった麻薬から何というでしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: アヘン戦争 (Opium War)\n",
      "Step 2 : Response language detection: English: 61.5%, Japanese: 38.5%\n",
      "<answer>Answer:アヘン戦争 (Opium War)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "気象用語で「平年」といえば、過去何年間の平均値のことでしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: 30年\n",
      "Step 2 : Response language detection: English: 66.7%, Japanese: 33.3%\n",
      "<answer>Answer:30年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1号は本郷猛、2号は一文字隼人がその正体である、テレビヒーローといえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 仮面ライダー\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：仮面ライダー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本国憲法に定められている国民の三大義務とは、勤労、教育と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 納税\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：納税<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太棹、中棹、細棹といえば、どんな楽器の種類でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 三味線\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三味線<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "代表作に『日本の黒い霧』や『点と線』などがある、推理作家は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 松本清張\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：松本清張<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "料理のときに使う計量スプーンで、大さじ１杯といったら通常何ccでしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: 15cc\n",
      "Step 2 : Response language detection: English: 100.0%\n",
      "<answer>Answer:15cc<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "スキー板に靴を取り付けるための締め具のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ビンディング\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ビンディング<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オーストラリアの国旗に星は６つありますが、ニュージーランドの国旗には星がいくつあるでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ４つ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：４つ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1934年には天然記念物に指定された、学名を「ニッポニア・ニッポン」という鳥といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: トキ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：トキ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "火を神聖視するために「拝火教」の別名がある、紀元前６世紀のペルシアで始まった宗教は何教でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ゾロアスター教\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ゾロアスター教<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "テレビの視聴率調査における「ゴールデンタイム」とは、午後７時から何時までのことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 午後１０時\n",
      "ステップ 2 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：午後１０時<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "他には影響を及ぼさない、狭い範囲でのもめ事のことを、「何の中の嵐」というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: コップ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "７０１年に藤原不比等（ふじわらのふひと）らが長部親王（おさべ）らと共に作り上げた、国家の基本法典を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 大宝律令\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大宝律令<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "妊娠５ヶ月目の戌（いぬ）の日に体に付ける、胎児を保護するためにする帯を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 岩田帯\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：岩田帯<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "セルバンテスの小説『ドン・キホーテ』で、ドン・キホーテが巨人と間違えて襲いかかったものは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 風車\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：風車<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ボウリングの球の重さを表す単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ポンド\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポンド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "その色合いが鳩の血のように見えることから、最高級のルビーのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ピジョンブラッド\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピジョンブラッド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "箱根や富士などの火山から噴き出した灰が積もって形成された、広く関東地方をおおっている赤い粘土質の地層を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 関東ローム層\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：関東ローム層<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "地方から東京へ出ることを「上京」といいますが、地方から京都へ出ることを何といったでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 上洛\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：上洛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本の国宝第1号に認定された弥勒菩薩像がある、京都の寺はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 広隆寺\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：広隆寺<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "牛２頭で１日に耕した面積が起源といわれる、ヤード・ポンド法の面積の単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: エーカー\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エーカー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『学校』シリーズや、『男はつらいよ』シリーズで知られる映画監督といえば誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 山田洋次\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：山田洋次<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "いわゆる「四書五経」の「四書」とは、『大学』『孟子』『論語』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『中庸』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：『中庸』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "春の七草で「すずな」といえば蕪のことですが、「すずしろ」といえば何のことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 大根\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大根<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年の元日に決勝が行われたサッカー天皇杯で、初優勝を飾ったチームは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 京都パープルサンガ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：京都パープルサンガ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ことわざ「弘法も筆の誤り」で、弘法大師が間違えたとされる文字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 応\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：応<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "南米の国、ウルグアイとパラグアイは、共にどこの国の領土から独立したでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: スペイン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：スペイン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "北原白秋作詞の童謡『あわて床屋』の中で、床屋に行ったお客である動物は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ウサギ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ウサギ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『Tokyo Walker』や『Kansai Walker』などの、「Walker」シリーズを発行している出版社といえばどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : Understand the answer: 角川書店\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：角川書店<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "50㎞以上250㎞以下の短い路線を運行する「地域航空輸送」を、英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: コミューター航空\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コミューター航空<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロウソクの炎を3つに分けると外炎、内炎と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 炎心\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：炎心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つ以上の文字を組み合わせて図案化したもののことで、特にルイ・ヴィトンのものが知られるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: モノグラム\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：モノグラム<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "東京の落語家で、真打ちと前座の中間にあるランクを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 二ツ目\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：二ツ目<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大相撲の第６８代横綱・朝青龍は何部屋の力士でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 高砂部屋\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高砂部屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ギリシャ神話で、ろうで固めた翼で空を飛んだものの、太陽の熱でろうが溶けて墜落死したのは誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: イカロス\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イカロス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1924年以降、南アフリカの各地で発見され、ラテン語で「南の猿」という意味の名がつけられた化石人類は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アウストラロピテクス\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アウストラロピテクス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デモテープ、デモ行進のデモとは、どんな言葉を略したものでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: デモンストレーション\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：デモンストレーション<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "この時期を表した俳句「春の海 ひねもすのたり のたりかな」を詠んだ、江戸時代の俳人は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 与謝蕪村\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与謝蕪村<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシアの作曲家、チャイコフスキーの三大バレエ音楽といえば、『白鳥の湖』『くるみ割り人形』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『眠れる森の美女』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：『眠れる森の美女』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロイターといえばイギリスの通信社ですが、ＡＦＰといえばどこの国の通信社でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: フランス\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：フランス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ジャムやマーマレードを入れて飲む紅茶の飲み方を、ある国の名を取って何ティーというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ロシアンティー\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロシアンティー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリンピックの五輪の旗で、真ん中にある輪の色は何色でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 黒\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：黒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太陽系の惑星の中で衛星をもっていない2つの惑星とは、水星と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 金星\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：金星<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＡＢＢＡのヒットソングに綴ってストーリーが展開される、電通四季劇場「海」で公開中の劇団四季のミュージカルは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『マンマ・ミーア！』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 70.0%\n",
      "<answer>答え：『マンマ・ミーア！』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "エレベーターで使われる記号で、屋上を示す「Ｒ」はルーフの略ですが、地下を示す「Ｂ」はどんな単語の頭文字でしょう？（カタカナ可）<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ベースメント\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ベースメント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インドではイギリスが東インド会社を設立し、日本では関ケ原の戦いの火ぶたが切って落とされたのは、西暦何年のことでしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: 1600年\n",
      "Step 2 : Response language detection: English: 80.0%, Japanese: 20.0%\n",
      "<answer>Answer:1600年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "とっくりを持ったタヌキの置物が有名な、滋賀県の焼物といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 信楽（しがらき）焼\n",
      "ステップ 2 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：信楽（しがらき）焼<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本では上松美香（あげまつみか）がその演奏者として知られている、「インディアンハープ」とも言われる民族楽器は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アルパ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アルパ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカのブッシュ (George Walker Bush) 現大統領が昨年の一般教書演説で使って以来広まった、イラン・北朝鮮・イラクの批判的な総称は何でしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: 悪の枢軸 (Axis of Evil)\n",
      "Step 2 : Response language detection: English: 71.4%, Japanese: 28.6%\n",
      "<answer>Answer:悪の枢軸 (Axis of Evil)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "木下順二の戯曲『夕鶴』で、助けた鶴の化身・つうと結婚する男の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 与ひょう\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与ひょう<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "幾つかの布切れを縫いあわせ、飾りや模様を作る手芸方法を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: パッチワーク\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：パッチワーク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "紫外線ＵＶ－Ｂの作用によって起こる、皮膚が褐色になる健康的な日焼けを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: サンタン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：サンタン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカ合衆国で、一番面積が広い州はアラスカ州ですが、一番面積が狭い州はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ロードアイランド州\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロードアイランド州<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フェンシングの試合場のことを何というでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ピスト\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主著に『純粋理性批判』『実践理性批判』がある、ドイツ観念論の創始者である哲学者といえば誰でしょう?<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: イマニュエル・カント\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イマニュエル・カント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "態度や意見ががらりと変わってしまうことを、ある動物の斑紋が季節によって鮮やかに変わることから何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 豹変\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：豹変<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "英語の『ドレミの歌』の歌詞で、「自分を呼ぶ名前」と歌われている音は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ミ（Ｍｅ、Ｍｉ）\n",
      "ステップ 2 : 応答言語の検出: Japanese: 12.5%\n",
      "<answer>答え：ミ（Ｍｅ、Ｍｉ）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年秋に開幕する、ラグビーの社会人１２チームによる全国規模のリーグ戦を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: （ジャパンラグビー）トップリーグ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 87.5%\n",
      "<answer>答え：（ジャパンラグビー）トップリーグ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イスラム文化で用いられた、植物の茎や葉を組み合わせて作った幾何学文様や唐草文様を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アラベスク\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アラベスク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「郵政三事業の民営化」などというときの「三事業」とは、「郵便」「貯金」とあと１つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 簡易保険（保険は×）\n",
      "ステップ 2 : 応答言語の検出: Japanese: 70.0%, English: 10.0%\n",
      "<answer>答え：簡易保険（保険は×）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主に夏の夜に行われる､まきを焚いて野外で行う能のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 薪能(たきぎのう)\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：薪能(たきぎのう)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大きな火皿を持ち、吸い口までが湾曲したパイプを、船乗りがよく使ったことから何パイプというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: マドロスパイプ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：マドロスパイプ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "青銅といえば銅と錫の合金ですが、黄銅は銅と何との合金でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 亜鉛\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：亜鉛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "江戸時代の村役人で「村方三役」と呼ばれたのは、庄屋、組頭とあと一つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 百姓代\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：百姓代<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ハクチョウを県の鳥にしている２つの県といえば、青森県とどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 島根県\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：島根県<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ずばり、卓球は1セット何点先取でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 11点（昨年よりルール改正）\n",
      "ステップ 2 : 応答言語の検出: Japanese: 71.4%, English: 14.3%\n",
      "<answer>答え：11点（昨年よりルール改正）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＪＩＳ規格のキーボードでのタッチタイピングで、人差し指を置く基本となるキーは「Ｆ」と何でしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: Ｊ\n",
      "Step 2 : Response language detection: Unknown\n",
      "<answer>Answer:Ｊ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "映画『ロッキー』で、女優タリア・シャイアが演じた、ロッキーの妻の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: エイドリアン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エイドリアン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インクのしみを落として作った左右対称の図形が何に見えるかで診断する人格検査法を、スイスの精神医学者の名前を取って何テストというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ロールシャッハ・テスト\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロールシャッハ・テスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「行く河の流れは絶えずして、しかも元の水にあらず」という書き出しの、鴨長明の随筆といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『方丈記』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：『方丈記』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "嵐、アルフィー、ミスターチルドレンの３つのグループのメンバーに共通する名字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 桜井\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：桜井<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "立秋の後の暑さを「残暑」といいますが、立春の後の寒さのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 余寒\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：余寒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "出世魚ボラの成長しきった時の名に因んだ、結局のところ、という意味の言葉は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: とどのつまり\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：とどのつまり<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "カジノのルーレットにある数字の中で、最大の数字はいくつでしょう？<eos><bot><eot>\n",
      "Step 1 : 答えを理解する: 36\n",
      "Step 2 : Response language detection: English: 100.0%\n",
      "<answer>Answer:36<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フォークボールを武器に通算215勝を挙げ、｢フォークボールの神様｣の異名を取った中日の投手は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 杉下茂\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：杉下茂<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "チベットのラサにある、ダライ・ダマが住む宮殿といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ポタラ宮\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポタラ宮<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イギリスで中世の封建制度崩壊と共に現れた独立自営農民のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ヨーマン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ヨーマン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "小木博明(おぎ・ひろあき）と矢作兼（やはぎ・けん）の２人が組んでいる、お笑いコンビの名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: おぎやはぎ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：おぎやはぎ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "果物を切る時などに使われる小型のナイフのことを、和製英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ぺティナイフ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ぺティナイフ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "コーヒーはアカネ科の植物ですが、カカオは何科の植物でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アオギリ科\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アオギリ科<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デパートの「友の会」で、「クローバーサークル」は伊勢丹ですが、「ローズサークル」といえばどこのデパートでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 高島屋\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高島屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリコンのシングルチャートで初めて「デビュー曲初登場1位」となった、近藤真彦のヒット曲は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 『スニーカーぶる～す』\n",
      "ステップ 2 : 応答言語の検出: Japanese: 72.7%\n",
      "<answer>答え：『スニーカーぶる～す』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "年に２度収穫できる「二度芋」といえばジャガイモのことですが、年に３度収穫できることから「三度豆」と呼ばれているのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: インゲン豆\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：インゲン豆<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Stage: ========================>>>>>>>>>>>>>>>>> 3\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\n",
      "ステップ 2 : 答えを理解する: 26文字\n",
      "ステップ 3 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 3 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 格闘家ボブ・サップの出身国はどこでしょう？\n",
      "ステップ 2 : 答えを理解する: アメリカ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？\n",
      "ステップ 2 : 答えを理解する: クレムリン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？\n",
      "ステップ 2 : 答えを理解する: ホトトギス\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気タレント・タモリの本名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 人気タレント・タモリの本名は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 森田一義\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：森田一義<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つのサイコロをふったとき、ゾロ目が出る確率は何分のいくつでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ２つのサイコロをふったとき、ゾロ目が出る確率は何分のいくつでしょう？\n",
      "ステップ 2 : 答えを理解する: ６分の１\n",
      "ステップ 3 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：６分の１<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「国際連合」の旗に描かれている植物といったら何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「国際連合」の旗に描かれている植物といったら何でしょう？\n",
      "ステップ 2 : 答えを理解する: オリーブ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：オリーブ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アンデルセンの自叙伝とも言われている物語で、いじめられっこが実は白鳥だったというお話は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: アンデルセンの自叙伝とも言われている物語で、いじめられっこが実は白鳥だったというお話は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『みにくいあひるの子』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 81.8%\n",
      "<answer>答え：『みにくいあひるの子』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "男性の声の音域で、テノールとバスの間に当たるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 男性の声の音域で、テノールとバスの間に当たるのは何でしょう？\n",
      "ステップ 2 : 答えを理解する: バリトン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：バリトン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "中華料理の飲茶で使われる、軽食やお菓子のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 中華料理の飲茶で使われる、軽食やお菓子のことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 点心\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：点心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「たけやぶやけた」などのように、前から読んでも後ろから読んでも同じになる文章を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「たけやぶやけた」などのように、前から読んでも後ろから読んでも同じになる文章を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 回文\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：回文<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ゴルフボールの表面につけられているくぼみのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ゴルフボールの表面につけられているくぼみのことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: ディンプル\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ディンプル<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "別名を『ラ・ジョコンダ』という、レオナルド・ダ・ヴィンチが描いた絵画といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 別名を『ラ・ジョコンダ』という、レオナルド・ダ・ヴィンチが描いた絵画といえば何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『モナ・リザ』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 71.4%\n",
      "<answer>答え：『モナ・リザ』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「東京湾」は２つの半島に囲まれていますが、それは房総半島と何半島でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「東京湾」は２つの半島に囲まれていますが、それは房総半島と何半島でしょう？\n",
      "ステップ 2 : 答えを理解する: 三浦半島\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三浦半島<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "市販の自動車に手を加えて、性能を高めることを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 市販の自動車に手を加えて、性能を高めることを英語で何というでしょう？\n",
      "ステップ 2 : 答えを理解する: チューンナップ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：チューンナップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1840年に勃発し、香港がイギリスに割譲される結果に終わった戦争を、原因となった麻薬から何というでしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: 1840年に勃発し、香港がイギリスに割譲される結果に終わった戦争を、原因となった麻薬から何というでしょう？\n",
      "Step 2 : 答えを理解する: アヘン戦争 (Opium War)\n",
      "Step 3 : Response language detection: English: 61.5%, Japanese: 38.5%\n",
      "<answer>Answer:アヘン戦争 (Opium War)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "気象用語で「平年」といえば、過去何年間の平均値のことでしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: 気象用語で「平年」といえば、過去何年間の平均値のことでしょう？\n",
      "Step 2 : 答えを理解する: 30年\n",
      "Step 3 : Response language detection: English: 66.7%, Japanese: 33.3%\n",
      "<answer>Answer:30年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1号は本郷猛、2号は一文字隼人がその正体である、テレビヒーローといえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 1号は本郷猛、2号は一文字隼人がその正体である、テレビヒーローといえば何でしょう？\n",
      "ステップ 2 : 答えを理解する: 仮面ライダー\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：仮面ライダー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本国憲法に定められている国民の三大義務とは、勤労、教育と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 日本国憲法に定められている国民の三大義務とは、勤労、教育と何でしょう？\n",
      "ステップ 2 : 答えを理解する: 納税\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：納税<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太棹、中棹、細棹といえば、どんな楽器の種類でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 太棹、中棹、細棹といえば、どんな楽器の種類でしょう？\n",
      "ステップ 2 : 答えを理解する: 三味線\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：三味線<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "代表作に『日本の黒い霧』や『点と線』などがある、推理作家は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 代表作に『日本の黒い霧』や『点と線』などがある、推理作家は誰でしょう？\n",
      "ステップ 2 : 答えを理解する: 松本清張\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：松本清張<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "料理のときに使う計量スプーンで、大さじ１杯といったら通常何ccでしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: 料理のときに使う計量スプーンで、大さじ１杯といったら通常何ccでしょう？\n",
      "Step 2 : 答えを理解する: 15cc\n",
      "Step 3 : Response language detection: English: 100.0%\n",
      "<answer>Answer:15cc<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "スキー板に靴を取り付けるための締め具のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: スキー板に靴を取り付けるための締め具のことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: ビンディング\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ビンディング<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オーストラリアの国旗に星は６つありますが、ニュージーランドの国旗には星がいくつあるでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: オーストラリアの国旗に星は６つありますが、ニュージーランドの国旗には星がいくつあるでしょう？\n",
      "ステップ 2 : 答えを理解する: ４つ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：４つ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1934年には天然記念物に指定された、学名を「ニッポニア・ニッポン」という鳥といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 1934年には天然記念物に指定された、学名を「ニッポニア・ニッポン」という鳥といえば何でしょう？\n",
      "ステップ 2 : 答えを理解する: トキ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：トキ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "火を神聖視するために「拝火教」の別名がある、紀元前６世紀のペルシアで始まった宗教は何教でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 火を神聖視するために「拝火教」の別名がある、紀元前６世紀のペルシアで始まった宗教は何教でしょう？\n",
      "ステップ 2 : 答えを理解する: ゾロアスター教\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ゾロアスター教<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "テレビの視聴率調査における「ゴールデンタイム」とは、午後７時から何時までのことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: テレビの視聴率調査における「ゴールデンタイム」とは、午後７時から何時までのことでしょう？\n",
      "ステップ 2 : 答えを理解する: 午後１０時\n",
      "ステップ 3 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：午後１０時<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "他には影響を及ぼさない、狭い範囲でのもめ事のことを、「何の中の嵐」というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 他には影響を及ぼさない、狭い範囲でのもめ事のことを、「何の中の嵐」というでしょう？\n",
      "ステップ 2 : 答えを理解する: コップ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コップ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "７０１年に藤原不比等（ふじわらのふひと）らが長部親王（おさべ）らと共に作り上げた、国家の基本法典を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ７０１年に藤原不比等（ふじわらのふひと）らが長部親王（おさべ）らと共に作り上げた、国家の基本法典を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 大宝律令\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大宝律令<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "妊娠５ヶ月目の戌（いぬ）の日に体に付ける、胎児を保護するためにする帯を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 妊娠５ヶ月目の戌（いぬ）の日に体に付ける、胎児を保護するためにする帯を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 岩田帯\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：岩田帯<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "セルバンテスの小説『ドン・キホーテ』で、ドン・キホーテが巨人と間違えて襲いかかったものは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: セルバンテスの小説『ドン・キホーテ』で、ドン・キホーテが巨人と間違えて襲いかかったものは何でしょう？\n",
      "ステップ 2 : 答えを理解する: 風車\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：風車<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ボウリングの球の重さを表す単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ボウリングの球の重さを表す単位は何でしょう？\n",
      "ステップ 2 : 答えを理解する: ポンド\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポンド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "その色合いが鳩の血のように見えることから、最高級のルビーのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: その色合いが鳩の血のように見えることから、最高級のルビーのことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: ピジョンブラッド\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピジョンブラッド<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "箱根や富士などの火山から噴き出した灰が積もって形成された、広く関東地方をおおっている赤い粘土質の地層を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 箱根や富士などの火山から噴き出した灰が積もって形成された、広く関東地方をおおっている赤い粘土質の地層を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 関東ローム層\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：関東ローム層<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "地方から東京へ出ることを「上京」といいますが、地方から京都へ出ることを何といったでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 地方から東京へ出ることを「上京」といいますが、地方から京都へ出ることを何といったでしょう？\n",
      "ステップ 2 : 答えを理解する: 上洛\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：上洛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本の国宝第1号に認定された弥勒菩薩像がある、京都の寺はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 日本の国宝第1号に認定された弥勒菩薩像がある、京都の寺はどこでしょう?\n",
      "ステップ 2 : 答えを理解する: 広隆寺\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：広隆寺<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "牛２頭で１日に耕した面積が起源といわれる、ヤード・ポンド法の面積の単位は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 牛２頭で１日に耕した面積が起源といわれる、ヤード・ポンド法の面積の単位は何でしょう？\n",
      "ステップ 2 : 答えを理解する: エーカー\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エーカー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『学校』シリーズや、『男はつらいよ』シリーズで知られる映画監督といえば誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 『学校』シリーズや、『男はつらいよ』シリーズで知られる映画監督といえば誰でしょう？\n",
      "ステップ 2 : 答えを理解する: 山田洋次\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：山田洋次<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "いわゆる「四書五経」の「四書」とは、『大学』『孟子』『論語』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: いわゆる「四書五経」の「四書」とは、『大学』『孟子』『論語』と何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『中庸』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 50.0%\n",
      "<answer>答え：『中庸』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "春の七草で「すずな」といえば蕪のことですが、「すずしろ」といえば何のことでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 春の七草で「すずな」といえば蕪のことですが、「すずしろ」といえば何のことでしょう？\n",
      "ステップ 2 : 答えを理解する: 大根\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：大根<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年の元日に決勝が行われたサッカー天皇杯で、初優勝を飾ったチームは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 今年の元日に決勝が行われたサッカー天皇杯で、初優勝を飾ったチームは何でしょう？\n",
      "ステップ 2 : 答えを理解する: 京都パープルサンガ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：京都パープルサンガ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ことわざ「弘法も筆の誤り」で、弘法大師が間違えたとされる文字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ことわざ「弘法も筆の誤り」で、弘法大師が間違えたとされる文字は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 応\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：応<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "南米の国、ウルグアイとパラグアイは、共にどこの国の領土から独立したでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 南米の国、ウルグアイとパラグアイは、共にどこの国の領土から独立したでしょう？\n",
      "ステップ 2 : 答えを理解する: スペイン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：スペイン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "北原白秋作詞の童謡『あわて床屋』の中で、床屋に行ったお客である動物は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 北原白秋作詞の童謡『あわて床屋』の中で、床屋に行ったお客である動物は何でしょう？\n",
      "ステップ 2 : 答えを理解する: ウサギ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ウサギ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "『Tokyo Walker』や『Kansai Walker』などの、「Walker」シリーズを発行している出版社といえばどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : Understand the question: 『Tokyo Walker』や『Kansai Walker』などの、「Walker」シリーズを発行している出版社といえばどこでしょう？\n",
      "ステップ 2 : Understand the answer: 角川書店\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：角川書店<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "50㎞以上250㎞以下の短い路線を運行する「地域航空輸送」を、英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 50㎞以上250㎞以下の短い路線を運行する「地域航空輸送」を、英語で何というでしょう？\n",
      "ステップ 2 : 答えを理解する: コミューター航空\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：コミューター航空<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロウソクの炎を3つに分けると外炎、内炎と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ロウソクの炎を3つに分けると外炎、内炎と何でしょう？\n",
      "ステップ 2 : 答えを理解する: 炎心\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：炎心<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "２つ以上の文字を組み合わせて図案化したもののことで、特にルイ・ヴィトンのものが知られるのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ２つ以上の文字を組み合わせて図案化したもののことで、特にルイ・ヴィトンのものが知られるのは何でしょう？\n",
      "ステップ 2 : 答えを理解する: モノグラム\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：モノグラム<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "東京の落語家で、真打ちと前座の中間にあるランクを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 東京の落語家で、真打ちと前座の中間にあるランクを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 二ツ目\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：二ツ目<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大相撲の第６８代横綱・朝青龍は何部屋の力士でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 大相撲の第６８代横綱・朝青龍は何部屋の力士でしょう？\n",
      "ステップ 2 : 答えを理解する: 高砂部屋\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高砂部屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ギリシャ神話で、ろうで固めた翼で空を飛んだものの、太陽の熱でろうが溶けて墜落死したのは誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ギリシャ神話で、ろうで固めた翼で空を飛んだものの、太陽の熱でろうが溶けて墜落死したのは誰でしょう？\n",
      "ステップ 2 : 答えを理解する: イカロス\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イカロス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "1924年以降、南アフリカの各地で発見され、ラテン語で「南の猿」という意味の名がつけられた化石人類は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 1924年以降、南アフリカの各地で発見され、ラテン語で「南の猿」という意味の名がつけられた化石人類は何でしょう？\n",
      "ステップ 2 : 答えを理解する: アウストラロピテクス\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アウストラロピテクス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デモテープ、デモ行進のデモとは、どんな言葉を略したものでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: デモテープ、デモ行進のデモとは、どんな言葉を略したものでしょう？\n",
      "ステップ 2 : 答えを理解する: デモンストレーション\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：デモンストレーション<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "この時期を表した俳句「春の海 ひねもすのたり のたりかな」を詠んだ、江戸時代の俳人は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: この時期を表した俳句「春の海 ひねもすのたり のたりかな」を詠んだ、江戸時代の俳人は誰でしょう？\n",
      "ステップ 2 : 答えを理解する: 与謝蕪村\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与謝蕪村<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシアの作曲家、チャイコフスキーの三大バレエ音楽といえば、『白鳥の湖』『くるみ割り人形』と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ロシアの作曲家、チャイコフスキーの三大バレエ音楽といえば、『白鳥の湖』『くるみ割り人形』と何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『眠れる森の美女』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：『眠れる森の美女』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロイターといえばイギリスの通信社ですが、ＡＦＰといえばどこの国の通信社でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ロイターといえばイギリスの通信社ですが、ＡＦＰといえばどこの国の通信社でしょう？\n",
      "ステップ 2 : 答えを理解する: フランス\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：フランス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ジャムやマーマレードを入れて飲む紅茶の飲み方を、ある国の名を取って何ティーというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ジャムやマーマレードを入れて飲む紅茶の飲み方を、ある国の名を取って何ティーというでしょう？\n",
      "ステップ 2 : 答えを理解する: ロシアンティー\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロシアンティー<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリンピックの五輪の旗で、真ん中にある輪の色は何色でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: オリンピックの五輪の旗で、真ん中にある輪の色は何色でしょう？\n",
      "ステップ 2 : 答えを理解する: 黒\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：黒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "太陽系の惑星の中で衛星をもっていない2つの惑星とは、水星と何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 太陽系の惑星の中で衛星をもっていない2つの惑星とは、水星と何でしょう？\n",
      "ステップ 2 : 答えを理解する: 金星\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：金星<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＡＢＢＡのヒットソングに綴ってストーリーが展開される、電通四季劇場「海」で公開中の劇団四季のミュージカルは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ＡＢＢＡのヒットソングに綴ってストーリーが展開される、電通四季劇場「海」で公開中の劇団四季のミュージカルは何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『マンマ・ミーア！』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 70.0%\n",
      "<answer>答え：『マンマ・ミーア！』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "エレベーターで使われる記号で、屋上を示す「Ｒ」はルーフの略ですが、地下を示す「Ｂ」はどんな単語の頭文字でしょう？（カタカナ可）<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: エレベーターで使われる記号で、屋上を示す「Ｒ」はルーフの略ですが、地下を示す「Ｂ」はどんな単語の頭文字でしょう？（カタカナ可）\n",
      "ステップ 2 : 答えを理解する: ベースメント\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ベースメント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インドではイギリスが東インド会社を設立し、日本では関ケ原の戦いの火ぶたが切って落とされたのは、西暦何年のことでしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: インドではイギリスが東インド会社を設立し、日本では関ケ原の戦いの火ぶたが切って落とされたのは、西暦何年のことでしょう？\n",
      "Step 2 : 答えを理解する: 1600年\n",
      "Step 3 : Response language detection: English: 80.0%, Japanese: 20.0%\n",
      "<answer>Answer:1600年<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "とっくりを持ったタヌキの置物が有名な、滋賀県の焼物といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: とっくりを持ったタヌキの置物が有名な、滋賀県の焼物といえば何でしょう？\n",
      "ステップ 2 : 答えを理解する: 信楽（しがらき）焼\n",
      "ステップ 3 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：信楽（しがらき）焼<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "日本では上松美香（あげまつみか）がその演奏者として知られている、「インディアンハープ」とも言われる民族楽器は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 日本では上松美香（あげまつみか）がその演奏者として知られている、「インディアンハープ」とも言われる民族楽器は何でしょう？\n",
      "ステップ 2 : 答えを理解する: アルパ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アルパ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカのブッシュ (George Walker Bush) 現大統領が昨年の一般教書演説で使って以来広まった、イラン・北朝鮮・イラクの批判的な総称は何でしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: アメリカのブッシュ (George Walker Bush) 現大統領が昨年の一般教書演説で使って以来広まった、イラン・北朝鮮・イラクの批判的な総称は何でしょう？\n",
      "Step 2 : 答えを理解する: 悪の枢軸 (Axis of Evil)\n",
      "Step 3 : Response language detection: English: 71.4%, Japanese: 28.6%\n",
      "<answer>Answer:悪の枢軸 (Axis of Evil)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "木下順二の戯曲『夕鶴』で、助けた鶴の化身・つうと結婚する男の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 木下順二の戯曲『夕鶴』で、助けた鶴の化身・つうと結婚する男の名前は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 与ひょう\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：与ひょう<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "幾つかの布切れを縫いあわせ、飾りや模様を作る手芸方法を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 幾つかの布切れを縫いあわせ、飾りや模様を作る手芸方法を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: パッチワーク\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：パッチワーク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "紫外線ＵＶ－Ｂの作用によって起こる、皮膚が褐色になる健康的な日焼けを英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 紫外線ＵＶ－Ｂの作用によって起こる、皮膚が褐色になる健康的な日焼けを英語で何というでしょう？\n",
      "ステップ 2 : 答えを理解する: サンタン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：サンタン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "アメリカ合衆国で、一番面積が広い州はアラスカ州ですが、一番面積が狭い州はどこでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: アメリカ合衆国で、一番面積が広い州はアラスカ州ですが、一番面積が狭い州はどこでしょう?\n",
      "ステップ 2 : 答えを理解する: ロードアイランド州\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロードアイランド州<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フェンシングの試合場のことを何というでしょう?<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: フェンシングの試合場のことを何というでしょう?\n",
      "ステップ 2 : 答えを理解する: ピスト\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ピスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主著に『純粋理性批判』『実践理性批判』がある、ドイツ観念論の創始者である哲学者といえば誰でしょう?<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 主著に『純粋理性批判』『実践理性批判』がある、ドイツ観念論の創始者である哲学者といえば誰でしょう?\n",
      "ステップ 2 : 答えを理解する: イマニュエル・カント\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：イマニュエル・カント<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "態度や意見ががらりと変わってしまうことを、ある動物の斑紋が季節によって鮮やかに変わることから何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 態度や意見ががらりと変わってしまうことを、ある動物の斑紋が季節によって鮮やかに変わることから何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 豹変\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：豹変<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "英語の『ドレミの歌』の歌詞で、「自分を呼ぶ名前」と歌われている音は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 英語の『ドレミの歌』の歌詞で、「自分を呼ぶ名前」と歌われている音は何でしょう？\n",
      "ステップ 2 : 答えを理解する: ミ（Ｍｅ、Ｍｉ）\n",
      "ステップ 3 : 応答言語の検出: Japanese: 12.5%\n",
      "<answer>答え：ミ（Ｍｅ、Ｍｉ）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "今年秋に開幕する、ラグビーの社会人１２チームによる全国規模のリーグ戦を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 今年秋に開幕する、ラグビーの社会人１２チームによる全国規模のリーグ戦を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: （ジャパンラグビー）トップリーグ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 87.5%\n",
      "<answer>答え：（ジャパンラグビー）トップリーグ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イスラム文化で用いられた、植物の茎や葉を組み合わせて作った幾何学文様や唐草文様を何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: イスラム文化で用いられた、植物の茎や葉を組み合わせて作った幾何学文様や唐草文様を何というでしょう？\n",
      "ステップ 2 : 答えを理解する: アラベスク\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アラベスク<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「郵政三事業の民営化」などというときの「三事業」とは、「郵便」「貯金」とあと１つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「郵政三事業の民営化」などというときの「三事業」とは、「郵便」「貯金」とあと１つは何でしょう？\n",
      "ステップ 2 : 答えを理解する: 簡易保険（保険は×）\n",
      "ステップ 3 : 応答言語の検出: Japanese: 70.0%, English: 10.0%\n",
      "<answer>答え：簡易保険（保険は×）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "主に夏の夜に行われる､まきを焚いて野外で行う能のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 主に夏の夜に行われる､まきを焚いて野外で行う能のことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 薪能(たきぎのう)\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：薪能(たきぎのう)<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "大きな火皿を持ち、吸い口までが湾曲したパイプを、船乗りがよく使ったことから何パイプというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 大きな火皿を持ち、吸い口までが湾曲したパイプを、船乗りがよく使ったことから何パイプというでしょう？\n",
      "ステップ 2 : 答えを理解する: マドロスパイプ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：マドロスパイプ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "青銅といえば銅と錫の合金ですが、黄銅は銅と何との合金でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 青銅といえば銅と錫の合金ですが、黄銅は銅と何との合金でしょう？\n",
      "ステップ 2 : 答えを理解する: 亜鉛\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：亜鉛<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "江戸時代の村役人で「村方三役」と呼ばれたのは、庄屋、組頭とあと一つは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 江戸時代の村役人で「村方三役」と呼ばれたのは、庄屋、組頭とあと一つは何でしょう？\n",
      "ステップ 2 : 答えを理解する: 百姓代\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：百姓代<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ハクチョウを県の鳥にしている２つの県といえば、青森県とどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ハクチョウを県の鳥にしている２つの県といえば、青森県とどこでしょう？\n",
      "ステップ 2 : 答えを理解する: 島根県\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：島根県<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ずばり、卓球は1セット何点先取でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ずばり、卓球は1セット何点先取でしょう？\n",
      "ステップ 2 : 答えを理解する: 11点（昨年よりルール改正）\n",
      "ステップ 3 : 応答言語の検出: Japanese: 71.4%, English: 14.3%\n",
      "<answer>答え：11点（昨年よりルール改正）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ＪＩＳ規格のキーボードでのタッチタイピングで、人差し指を置く基本となるキーは「Ｆ」と何でしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: ＪＩＳ規格のキーボードでのタッチタイピングで、人差し指を置く基本となるキーは「Ｆ」と何でしょう？\n",
      "Step 2 : 答えを理解する: Ｊ\n",
      "Step 3 : Response language detection: Unknown\n",
      "<answer>Answer:Ｊ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "映画『ロッキー』で、女優タリア・シャイアが演じた、ロッキーの妻の名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 映画『ロッキー』で、女優タリア・シャイアが演じた、ロッキーの妻の名前は何でしょう？\n",
      "ステップ 2 : 答えを理解する: エイドリアン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：エイドリアン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "インクのしみを落として作った左右対称の図形が何に見えるかで診断する人格検査法を、スイスの精神医学者の名前を取って何テストというでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: インクのしみを落として作った左右対称の図形が何に見えるかで診断する人格検査法を、スイスの精神医学者の名前を取って何テストというでしょう？\n",
      "ステップ 2 : 答えを理解する: ロールシャッハ・テスト\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ロールシャッハ・テスト<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "「行く河の流れは絶えずして、しかも元の水にあらず」という書き出しの、鴨長明の随筆といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「行く河の流れは絶えずして、しかも元の水にあらず」という書き出しの、鴨長明の随筆といえば何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『方丈記』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 60.0%\n",
      "<answer>答え：『方丈記』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "嵐、アルフィー、ミスターチルドレンの３つのグループのメンバーに共通する名字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 嵐、アルフィー、ミスターチルドレンの３つのグループのメンバーに共通する名字は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 桜井\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：桜井<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "立秋の後の暑さを「残暑」といいますが、立春の後の寒さのことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 立秋の後の暑さを「残暑」といいますが、立春の後の寒さのことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: 余寒\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：余寒<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "出世魚ボラの成長しきった時の名に因んだ、結局のところ、という意味の言葉は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 出世魚ボラの成長しきった時の名に因んだ、結局のところ、という意味の言葉は何でしょう？\n",
      "ステップ 2 : 答えを理解する: とどのつまり\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：とどのつまり<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "カジノのルーレットにある数字の中で、最大の数字はいくつでしょう？<eos><bot><eot>\n",
      "Step 1 : 質問を理解する: カジノのルーレットにある数字の中で、最大の数字はいくつでしょう？\n",
      "Step 2 : 答えを理解する: 36\n",
      "Step 3 : Response language detection: English: 100.0%\n",
      "<answer>Answer:36<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "フォークボールを武器に通算215勝を挙げ、｢フォークボールの神様｣の異名を取った中日の投手は誰でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: フォークボールを武器に通算215勝を挙げ、｢フォークボールの神様｣の異名を取った中日の投手は誰でしょう？\n",
      "ステップ 2 : 答えを理解する: 杉下茂\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：杉下茂<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "チベットのラサにある、ダライ・ダマが住む宮殿といえば何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: チベットのラサにある、ダライ・ダマが住む宮殿といえば何でしょう？\n",
      "ステップ 2 : 答えを理解する: ポタラ宮\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ポタラ宮<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "イギリスで中世の封建制度崩壊と共に現れた独立自営農民のことを何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: イギリスで中世の封建制度崩壊と共に現れた独立自営農民のことを何というでしょう？\n",
      "ステップ 2 : 答えを理解する: ヨーマン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ヨーマン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "小木博明(おぎ・ひろあき）と矢作兼（やはぎ・けん）の２人が組んでいる、お笑いコンビの名前は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 小木博明(おぎ・ひろあき）と矢作兼（やはぎ・けん）の２人が組んでいる、お笑いコンビの名前は何でしょう？\n",
      "ステップ 2 : 答えを理解する: おぎやはぎ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：おぎやはぎ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "果物を切る時などに使われる小型のナイフのことを、和製英語で何というでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 果物を切る時などに使われる小型のナイフのことを、和製英語で何というでしょう？\n",
      "ステップ 2 : 答えを理解する: ぺティナイフ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ぺティナイフ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "コーヒーはアカネ科の植物ですが、カカオは何科の植物でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: コーヒーはアカネ科の植物ですが、カカオは何科の植物でしょう？\n",
      "ステップ 2 : 答えを理解する: アオギリ科\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アオギリ科<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "デパートの「友の会」で、「クローバーサークル」は伊勢丹ですが、「ローズサークル」といえばどこのデパートでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: デパートの「友の会」で、「クローバーサークル」は伊勢丹ですが、「ローズサークル」といえばどこのデパートでしょう？\n",
      "ステップ 2 : 答えを理解する: 高島屋\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：高島屋<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "オリコンのシングルチャートで初めて「デビュー曲初登場1位」となった、近藤真彦のヒット曲は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: オリコンのシングルチャートで初めて「デビュー曲初登場1位」となった、近藤真彦のヒット曲は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 『スニーカーぶる～す』\n",
      "ステップ 3 : 応答言語の検出: Japanese: 72.7%\n",
      "<answer>答え：『スニーカーぶる～す』<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "年に２度収穫できる「二度芋」といえばジャガイモのことですが、年に３度収穫できることから「三度豆」と呼ばれているのは何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 年に２度収穫できる「二度芋」といえばジャガイモのことですが、年に３度収穫できることから「三度豆」と呼ばれているのは何でしょう？\n",
      "ステップ 2 : 答えを理解する: インゲン豆\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：インゲン豆<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "truncated_dataset = DatasetDict({\n",
    "    split: dataset[split].select(range(100))\n",
    "    for split in dataset.keys()\n",
    "})\n",
    "\n",
    "for stage in range(config[\"stages\"]):\n",
    "    dataset_ = truncated_dataset.map(\n",
    "        (lambda x: preprocess_function(\n",
    "            x, \n",
    "            detector=detector,\n",
    "            stages=stage, \n",
    "            language_config=language_config\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Stage: ========================>>>>>>>>>>>>>>>>> {stage}\")\n",
    "    for i in range(100):\n",
    "        print(\"Input: \", dataset_['train'][\"prompt\"][i], \"\\n\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Modelling\n",
    "\n",
    "Our `LatentReasoningGemmaForCausalLM` extends GemmaForCausalLM to enable continuous latent reasoning, following the architecture from Hao et al. (2024). The model implements two key forward paths:\n",
    "\n",
    "`infer_forward`: During inference, transforms input text into continuous thought representations before generating the final output. It maintains a chain of latent states between the `<bot>` and `<eot>` tokens, allowing for more nuanced reasoning across languages.\n",
    "\n",
    "`train_forward`: During training, processes the sequence in stages, gradually building up continuous thought representations while masking appropriate parts of the input. It helps the model learn to reason in latent space while maintaining language understanding.\n",
    "\n",
    "Let's proceed with the creation of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 10:41:03.521708: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-30 10:41:03.535048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735526463.552361    1793 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735526463.557441    1793 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-30 10:41:03.576654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GemmaForCausalLM, DynamicCache, PreTrainedTokenizer\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LatentReasoningGemmaForCausalLM(GemmaForCausalLM):\n",
    "    \"\"\"\n",
    "    A custom implementation of GemmaForCausalLM that supports latent reasoning \n",
    "    using the Coconut (Chain of Continuous Thought) paradigm.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_CONFIG = {\n",
    "        # Core Learning Parameters\n",
    "        \"continuous_thoughts\": 4,               # Number of latent space reasoning steps\n",
    "        \"stages\": 4,                            # Number of training curriculum stages\n",
    "        \"training_thoughts_sequence_length\": 50, # Number of thought sequence to generate\n",
    "\n",
    "        # Inference and Evaluation Params       \n",
    "        \"fuzzy_matcher_threshold\": 80,          # Fuzzy matcher threshold at 80%\n",
    "        \"cot_decoding_k\": 5,                    # Number of paths to try before finding the best answer\n",
    "\n",
    "        # Model Setup\n",
    "        \"max_length\": 256,                      # Maximum text length to process\n",
    "\n",
    "        # Special Tokens\n",
    "        \"bot_id\": \"<bot>\",                      # Marks start of latent reasoning\n",
    "        \"eot_id\": \"<eot>\",                      # Marks end of latent reasoning\n",
    "        \"answer_id\": \"<answer>\",                # Marks the begining of answer\n",
    "        \"debug\": True,                          # Enables debugging output. Also allows you see the model's thoughts\n",
    "\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config_):\n",
    "        super().__init__(config_)\n",
    "        self.tokenizer: PreTrainedTokenizer = None\n",
    "        self.current_stage = 0\n",
    "        self.model_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "        self.debug = self.model_config.get(\"debug\", False)\n",
    "        self.diversity_weight = self.model_config.get(\"diversity_weight\", 0.1)\n",
    "        self.coherence_weight = self.model_config.get(\"coherence_weight\", 0.1)\n",
    "\n",
    "    def get_input_ids(self, inputs_embeds):\n",
    "        \"\"\"Helper method to get input ids from embeddings.\"\"\"\n",
    "        embedding_matrix = self.get_input_embeddings().weight\n",
    "        similarities = torch.matmul(inputs_embeds, embedding_matrix.T)\n",
    "        token_ids = torch.argmax(similarities, dim=-1)\n",
    "        return token_ids\n",
    "\n",
    "    def thoughts_forward(self, num_thoughts, thought_ids, thought_mask, num_of_thought_tokens = 1):\n",
    "        \"\"\"\n",
    "        Generate continuous thought embeddings.\n",
    "        \"\"\"\n",
    "        all_thought_outputs = []\n",
    "        batch_size = thought_ids.shape[0]\n",
    "        \n",
    "        # Get initial embeddings\n",
    "        initial_embeds = self.get_input_embeddings()(thought_ids)\n",
    "        current_embeds = initial_embeds\n",
    "        current_mask = thought_mask\n",
    "\n",
    "        for t in range(num_thoughts):\n",
    "            # Forward pass through transformer\n",
    "            outputs = self.model.forward(\n",
    "                inputs_embeds=current_embeds,\n",
    "                attention_mask=current_mask,\n",
    "                past_key_values=None,\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,  # Get hidden states from all layers\n",
    "            )\n",
    "            \n",
    "            # Get hidden states from all layers for better representation\n",
    "            hidden_states = outputs.hidden_states\n",
    "            \n",
    "            # Combine hidden states from different layers using attention\n",
    "            layer_attention = torch.softmax(\n",
    "                torch.randn(len(hidden_states), device=hidden_states[0].device), \n",
    "                dim=0\n",
    "            )\n",
    "            weighted_states = sum(w * h for w, h in zip(layer_attention, hidden_states))\n",
    "            \n",
    "            n = num_of_thought_tokens\n",
    "            last_hidden = weighted_states[:, -n:, :]  # [batch_size, n, hidden_size]\n",
    "            \n",
    "            # Project to lower dimension for thought space\n",
    "            thought_proj = nn.Sequential(\n",
    "                nn.Linear(last_hidden.shape[-1], self.config.hidden_size // 2),\n",
    "                nn.LayerNorm(self.config.hidden_size // 2),\n",
    "                nn.GELU()\n",
    "            ).to(last_hidden.device)\n",
    "            projected_thought = thought_proj(last_hidden)  # [batch_size, n, hidden_size // 2]\n",
    "            \n",
    "            # Add noise to increase diversity\n",
    "            noise = torch.randn_like(projected_thought) * 0.1  # Adjust noise scale as needed\n",
    "            projected_thought = projected_thought + noise\n",
    "            \n",
    "            # Project back to embedding space\n",
    "            embed_proj = nn.Linear(\n",
    "                self.config.hidden_size // 2,\n",
    "                self.config.hidden_size,\n",
    "                device=projected_thought.device\n",
    "            )\n",
    "            next_token_embeds = embed_proj(projected_thought)  # [batch_size, n, hidden_size]\n",
    "            \n",
    "            # Apply layer normalization for stability\n",
    "            next_token_embeds = nn.LayerNorm(\n",
    "                self.config.hidden_size,\n",
    "                device=next_token_embeds.device\n",
    "            )(next_token_embeds)\n",
    "            \n",
    "            # Update embeddings and mask\n",
    "            current_embeds = torch.cat([current_embeds, next_token_embeds], dim=1)\n",
    "            current_mask = torch.cat([\n",
    "                current_mask,\n",
    "                torch.ones((batch_size, n), device=current_mask.device)\n",
    "            ], dim=1)\n",
    "            \n",
    "            all_thought_outputs.append(last_hidden)\n",
    "\n",
    "        # Ensure reasonable sequence length\n",
    "        max_seq_len = self.model_config.get(\"max_length\", 512)\n",
    "        if current_embeds.shape[1] > max_seq_len:\n",
    "            current_embeds = current_embeds[:, :max_seq_len, :]\n",
    "            current_mask = current_mask[:, :max_seq_len]\n",
    "        \n",
    "        return all_thought_outputs, current_embeds, current_mask\n",
    "\n",
    "\n",
    "    def train_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training forward pass with continuous thought generation and CoT alignment.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        # Keep original labels if none provided\n",
    "        if labels is None:\n",
    "            labels = input_ids.clone()\n",
    "            batch_size = labels.shape[0]\n",
    "            eot_id = self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"])\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find the positions of <eot> in the input_ids\n",
    "                eot_pos = (input_ids[i] == eot_id).nonzero(as_tuple=True)\n",
    "\n",
    "                if len(eot_pos[0]) > 0:\n",
    "                    # Get the last occurrence of <eot>\n",
    "                    last_eot_pos = eot_pos[0][-1].item()\n",
    "                    \n",
    "                    # Mask everything before and including the last <eot>\n",
    "                    labels[i, :last_eot_pos] = -100\n",
    "\n",
    "                # Mask padding\n",
    "                labels[i, attention_mask[i] == 0] = -100\n",
    "\n",
    "        # Get input embeddings if not provided\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # Generate continuous thoughts\n",
    "        if self.current_stage > 0:\n",
    "            num_thoughts = self.current_stage * self.model_config[\"continuous_thoughts\"]\n",
    "            all_thoughts, final_embeds, final_mask = self.thoughts_forward(\n",
    "                num_thoughts=num_thoughts,\n",
    "                thought_ids=input_ids,\n",
    "                thought_mask=attention_mask,\n",
    "                num_of_thought_tokens = self.model_config[\"training_thoughts_sequence_length\"]\n",
    "            )\n",
    "\n",
    "            # Add auxiliary losses\n",
    "            auxiliary_losses = []\n",
    "\n",
    "            # Thought coherence loss\n",
    "            if len(all_thoughts) > 1:\n",
    "                coherence_loss = 0\n",
    "                for t1, t2 in zip(all_thoughts[:-1], all_thoughts[1:]):\n",
    "                    sim = F.cosine_similarity(t1, t2, dim=-1)\n",
    "                    coherence_loss += (1 - sim).mean()\n",
    "                auxiliary_losses.append(coherence_loss * self.coherence_weight)\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find the start and end of CoT in the labels\n",
    "                cot_start = None\n",
    "                \n",
    "                for j, token_id in enumerate(labels[i]):\n",
    "                    if token_id == self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"]):\n",
    "                        cot_start = j + 1  # Start of CoT\n",
    "\n",
    "\n",
    "                # Debugging: Print CoT tokens and latent thoughts\n",
    "                if cot_start is not None:\n",
    "                    # Extract CoT tokens\n",
    "                    cot_tokens = labels[i, cot_start:]  # [cot_seq_len]\n",
    "\n",
    "                    # Get the latent thoughts for this batch\n",
    "                    latent_thoughts = all_thoughts[i]  # [thought_seq_len, hidden_size]\n",
    "\n",
    "                    # Project latent thoughts to logits\n",
    "                    thought_logits = self.lm_head(latent_thoughts)  # [thought_seq_len, vocab_size]\n",
    "                    thought_token_ids = torch.argmax(thought_logits, dim=-1)  # [thought_seq_len]\n",
    "\n",
    "\n",
    "                    # Debugging: Print CoT tokens and latent thoughts\n",
    "                    if self.debug:\n",
    "                        # Decode CoT tokens\n",
    "                        cot_tokens_list = cot_tokens.squeeze().tolist()  # Convert to 1D list\n",
    "                        if isinstance(cot_tokens_list, int):  # Handle single token case\n",
    "                            cot_tokens_list = [cot_tokens_list]\n",
    "                        cot_text = self.tokenizer.decode(cot_tokens_list, skip_special_tokens=True)\n",
    "                        print(f\" ==================== \\n Debug: CoT for batch {i}: {cot_text} \\n ====================\")\n",
    "\n",
    "                        # Decode latent thoughts\n",
    "                        thought_token_ids_list = thought_token_ids.squeeze().tolist()  # Convert to list\n",
    "\n",
    "                        # Ensure thought_token_ids_list is a flat list\n",
    "                        if isinstance(thought_token_ids_list, list) and all(isinstance(item, list) for item in thought_token_ids_list):\n",
    "                            # Flatten the nested list\n",
    "                            thought_token_ids_list = [token for sublist in thought_token_ids_list for token in sublist]\n",
    "                        elif isinstance(thought_token_ids_list, int):  # Handle single token case\n",
    "                            thought_token_ids_list = [thought_token_ids_list]\n",
    "\n",
    "                        # Decode the flat list of token IDs\n",
    "                        thought_text = self.tokenizer.decode(thought_token_ids_list, skip_special_tokens=False)\n",
    "                        print(f\"==================== \\n Debug: Latent thoughts for batch {i}: {thought_text} \\n ========================\")\n",
    "\n",
    "\n",
    "            # Forward pass with thoughts\n",
    "            outputs = super().forward(\n",
    "                inputs_embeds=final_embeds,\n",
    "                attention_mask=final_mask,\n",
    "                labels=labels,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            # Add auxiliary losses\n",
    "            if auxiliary_losses:\n",
    "                outputs.loss += sum(auxiliary_losses)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if inputs_embeds is None:\n",
    "                # Standard forward pass for initial stage\n",
    "                outputs = super().forward(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                outputs = super().forward(\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def infer_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[DynamicCache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inference forward pass with continuous thought generation.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # Insert <bot> token to initiate latent reasoning\n",
    "        if input_ids.shape[1] > 1:\n",
    "            input_ids = torch.cat(\n",
    "                [\n",
    "                    input_ids,\n",
    "                    torch.tensor(\n",
    "                        [[self.tokenizer.convert_tokens_to_ids(self.model_config[\"bot_id\"])]] * batch_size,\n",
    "                        device=input_ids.device,\n",
    "                    ),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            attention_mask = torch.cat(\n",
    "                [\n",
    "                    attention_mask,\n",
    "                    torch.ones((batch_size, 1), device=attention_mask.device),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Generate continuous thoughts\n",
    "        if self.model_config[\"stages\"] - 1 > 0 and input_ids.shape[1] > 1:\n",
    "            num_thoughts = (self.model_config[\"stages\"] - 1) * self.model_config[\"continuous_thoughts\"]\n",
    "            all_thoughts, final_embeds, final_mask = self.thoughts_forward(\n",
    "                num_thoughts, input_ids, attention_mask\n",
    "            )\n",
    "\n",
    "            # Add <eot> token to mark the end of latent reasoning\n",
    "            eot_embeds = self.get_input_embeddings()(\n",
    "                torch.tensor(\n",
    "                    [[self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"])]] * batch_size,\n",
    "                    device=final_embeds.device,\n",
    "                )\n",
    "            )\n",
    "            final_embeds = torch.cat([final_embeds, eot_embeds], dim=1)\n",
    "            final_mask = torch.cat([final_mask, torch.ones((batch_size, 1), device=final_mask.device)], dim=1)\n",
    "\n",
    "            # Generate final output in language mode\n",
    "            outputs = super().forward(\n",
    "                inputs_embeds=final_embeds,\n",
    "                attention_mask=final_mask,\n",
    "                past_key_values=None,  # Reset past_key_values for answer generation\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            # Standard forward pass (no latent thoughts)\n",
    "            outputs = super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[DynamicCache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Main forward function that routes to either training or inference.\"\"\"\n",
    "        forward_fn = self.train_forward if self.training else self.infer_forward\n",
    "        return forward_fn(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            num_logits_to_keep=num_logits_to_keep,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gemma2 to instantiate a model of type gemma. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1f1246867c4d7f8e00304ac0953b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215116f222da41949ae633e143212c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [config[\"bot_id\"], config[\"eot_id\"], config[\"answer_id\"]]\n",
    "}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load the Reasoning model configuration\n",
    "model_config = AutoConfig.from_pretrained(config[\"model_name\"])\n",
    "model = LatentReasoningGemmaForCausalLM(config_=model_config)\n",
    "\n",
    "# Load the Reasoning model\n",
    "model = model.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.tokenizer = tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Load the normal model for comparison\n",
    "model_without_reasoning = AutoModelForCausalLM.from_pretrained(config[\"model_name\"])\n",
    "model_without_reasoning.resize_token_embeddings(len(tokenizer))\n",
    "model_without_reasoning = model_without_reasoning.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create Helper functions for inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing Helper Function\n",
    "\n",
    "- Chain-of-Thought (CoT) Decoding: A technique where the model generates intermediate reasoning steps before producing the final answer.\n",
    "\n",
    "- Top-K Sampling: Selecting the k most likely tokens to explore multiple possible continuations.\n",
    "\n",
    "- Temperature: A parameter that controls the randomness of predictions. Higher values make the output more diverse, while lower values make it more deterministic.\n",
    "\n",
    "- Min-Margin Confidence: A measure of how confident the model is in its predictions, based on the difference between the best and second-best probabilities.\n",
    "\n",
    "\n",
    "Example Workflow:\n",
    "\n",
    "You ask a question: `What is the capital of France?`\n",
    "\n",
    "generate_answer explores multiple possible answers `(e.g., \"Paris\", \"London\", \"Berlin\")`.\n",
    "\n",
    "It calculates the confidence for each answer and selects the one with the highest confidence `(e.g., \"Paris\")`.\n",
    "\n",
    "The final answer \"Paris\" is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_answer(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    question: str,\n",
    "    max_length: int = 128,\n",
    "    k: int = config[\"cot_decoding_k\"],\n",
    "    temperature: float = 1.0,\n",
    "    **generation_kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates answer using CoT decoding and returns the best path.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        question: Input question\n",
    "        max_length: Maximum sequence length\n",
    "        k: Number of alternative paths to consider\n",
    "        temperature: Sampling temperature\n",
    "        **generation_kwargs: Additional generation arguments\n",
    "        \n",
    "    Returns:\n",
    "        Best decoded sequence with highest confidence\n",
    "    \"\"\"\n",
    "    # Initialize streamer\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(question, max_length=max_length, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Get initial logits for CoT paths\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    first_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    probs = F.softmax(first_token_logits, dim=-1)\n",
    "    top_k_probs, top_k_tokens = torch.topk(probs, k, dim=-1)\n",
    "    \n",
    "    best_path = None\n",
    "    best_confidence = -float('inf')\n",
    "    \n",
    "    # Generate continuation for each top-k token\n",
    "    for i in range(k):\n",
    "        # Prepare input with current top-k token\n",
    "        curr_input_ids = torch.cat([\n",
    "            input_ids,\n",
    "            top_k_tokens[:, i:i+1]\n",
    "        ], dim=1)\n",
    "        \n",
    "        curr_attention_mask = torch.cat([\n",
    "            attention_mask,\n",
    "            torch.ones((attention_mask.shape[0], 1), device=model.device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate with streamer for best path\n",
    "        outputs = model.generate(\n",
    "            input_ids=curr_input_ids,\n",
    "            attention_mask=curr_attention_mask,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            streamer=streamer if i == 0 else None,  # Only stream first path\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Calculate confidence for this path\n",
    "        _, confidence = calculate_answer_confidence(\n",
    "            outputs.sequences[0].tolist(),\n",
    "            outputs.scores[-1],\n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        # Update best path if confidence is higher\n",
    "        if confidence > best_confidence:\n",
    "            best_confidence = confidence\n",
    "            best_path = outputs.sequences[0]\n",
    "            \n",
    "    # Return the path with highest confidence\n",
    "    return tokenizer.decode(best_path, skip_special_tokens=True)\n",
    "\n",
    "def calculate_answer_confidence(\n",
    "    sequence: List[int],\n",
    "    final_logits: torch.Tensor,\n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"Calculate confidence score using min-margin approach.\"\"\"\n",
    "    # Extract answer from sequence\n",
    "    answer = extract_answer(sequence, tokenizer)\n",
    "    \n",
    "    if not answer:\n",
    "        return \"\", 0.0\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = F.softmax(final_logits, dim=-1)\n",
    "    \n",
    "    # Calculate margins for answer tokens\n",
    "    answer_tokens = tokenizer.encode(answer, add_special_tokens=False)\n",
    "    margins = []\n",
    "    \n",
    "    for token in answer_tokens:\n",
    "        token_prob = probs[0, token].item()\n",
    "        sorted_probs, _ = torch.sort(probs, dim=-1, descending=True)\n",
    "        second_best_prob = sorted_probs[0, 1].item()\n",
    "        margin = token_prob - second_best_prob\n",
    "        margins.append(margin)\n",
    "        \n",
    "    confidence = sum(margins) / len(margins)\n",
    "    return answer, confidence\n",
    "\n",
    "def extract_answer(sequence: List[int], tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Extract final answer from sequence using <eot> token.\n",
    "    Finds the answer between the last occurrence of <eot> and the end of sequence.\n",
    "    \"\"\"\n",
    "    # Convert sequence to string\n",
    "    decoded = tokenizer.decode(sequence)\n",
    "    \n",
    "    # Find last <eot> position\n",
    "    eot_position = decoded.rfind(config[\"eot_id\"])\n",
    "    \n",
    "    if eot_position != -1:\n",
    "        # Extract everything after the last <eot>\n",
    "        answer = decoded[eot_position + len(config[\"eot_id\"]):].strip()\n",
    "        return answer\n",
    "        \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tick_start = 0\n",
    "\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    "\n",
    "def text_gen(prompt, model, tokenizer):\n",
    "    tick()\n",
    "    input = f\"{prompt}\"\n",
    "    print(f\"Question: {prompt} \\n ==========================================\")\n",
    "    output = generate_answer(model=model, tokenizer=tokenizer, question=input, k=5, max_length=config[\"max_length\"] )\n",
    "    print(f\"Outputs: ========================\")\n",
    "    print(output)\n",
    "    tock()\n",
    "    print(f\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the capabilities of normal model before fine tuning the reasoning model.\n",
    "\n",
    "From the result, you'd notice it is not very good. It struggles with switching translation,  verbose is quite long and it takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？  \n",
      " ==========================================\n",
      "<bos>「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？ 答えは、26文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字\n",
      "Outputs: ========================\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？ 答えは、26文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字のアルファベットは、英語のアルファベットの全文字です。 26文字\n",
      "TOTAL TIME ELAPSED: 35.24s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Is the capital of Japan tokyo? \n",
      " ==========================================\n",
      "<bos>Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "\n",
      "Is the capital of Japan tokyo?\n",
      "Outputs: ========================\n",
      "Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital of Japan tokyo? Is the capital\n",
      "TOTAL TIME ELAPSED: 37.65s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Translate 'Hello, how are you?' to Japanese. \n",
      " ==========================================\n",
      "<bos>Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'Hello\n",
      "Outputs: ========================\n",
      "Translate 'Hello, how are you?' to Japanese. Hello, how are you?\n",
      "\n",
      "[Answer 1]\n",
      "\n",
      "    こんにちは、どうお過ごしですか？\n",
      "\n",
      "    Konnichiwa, dou osusugidesuka?\n",
      "\n",
      "    Hello, how are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you doing?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    \n",
      "TOTAL TIME ELAPSED: 29.92s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 「お元気ですか」を英語に訳すと \n",
      " ==========================================\n",
      "<bos>「お元気ですか」を英語に訳すと「Are you okay?」となります。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you well?」と訳す\n",
      "Outputs: ========================\n",
      "「お元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「\n",
      "TOTAL TIME ELAPSED: 36.77s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Translate to english `「ねえ、それは何のためにあるの？` \n",
      " ==========================================\n",
      "<bos>Translate to english `「ねえ、それは何のためにあるの？`\n",
      "\n",
      "[User 0001]\n",
      "\n",
      "Hello,\n",
      "\n",
      "I'm trying to translate this sentence:\n",
      "\n",
      "「ねえ、それは何のためにあるの？\n",
      "\n",
      "I'm not sure if it's a question or a statement.\n",
      "\n",
      "I'm thinking it's a question, but I'm not sure.\n",
      "\n",
      "Thanks!\n",
      " \n",
      "\n",
      "[User 0002]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0003]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0004]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0005]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0006]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0007]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0008]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0009]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0010]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0011\n",
      "Outputs: ========================\n",
      "Translate to english `「ねえ、それは何のためにあるの？`\n",
      "I'm not sure if this is the right place to ask this, but I'm trying to translate the following sentence: \"ねえ、それは何のためにあるの？\" I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it's a question or a statement, but I'm going to assume it's a question. I'm not sure if it'\n",
      "TOTAL TIME ELAPSED: 36.22s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the function\n",
    "text_gen(\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？ \", model=model_without_reasoning, tokenizer=tokenizer)\n",
    "text_gen(\"Is the capital of Japan tokyo?\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "text_gen(\"Translate 'Hello, how are you?' to Japanese.\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "text_gen(\"「お元気ですか」を英語に訳すと\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "text_gen(\"Translate to english `「ねえ、それは何のためにあるの？`\", model=model_without_reasoning, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7) Training\n",
    "\n",
    "The training implementation follows a multi-stage curriculum based on Hao et al. (2024), gradually introducing continuous latent reasoning. Each stage represents a step in transitioning from pure language processing to latent space reasoning:\n",
    "\n",
    "\n",
    "Using Hugging Face's Trainer with optimizations:\n",
    "- BFloat16 precision\n",
    "- 8-bit Adam optimizer \n",
    "- Gradient accumulation\n",
    "- WandB tracking\n",
    "- Checkpoint management\n",
    "\n",
    "The model progressively learns to leverage continuous thought states while preserving translation capabilities, with each stage building upon the previous one's learned representations.\n",
    "\n",
    "To train, you need to get ready your wandb token Id as we report the logs to wandb.\n",
    "To get your wandb key, visit [Wandb](https://wandb.ai/quickstart?utm_source=app-resource-center&utm_medium=app&utm_term=quickstart)\n",
    "\n",
    "Let's see this in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvicksemmanuel58\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/gemini-fine-tuning/wandb/run-20241230_104524-fs9ec50t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vicksemmanuel58/gemma2-finetuning/runs/fs9ec50t' target=\"_blank\">effortless-silence-15</a></strong> to <a href='https://wandb.ai/vicksemmanuel58/gemma2-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vicksemmanuel58/gemma2-finetuning' target=\"_blank\">https://wandb.ai/vicksemmanuel58/gemma2-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vicksemmanuel58/gemma2-finetuning/runs/fs9ec50t' target=\"_blank\">https://wandb.ai/vicksemmanuel58/gemma2-finetuning/runs/fs9ec50t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='808' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 808/9375 47:52 < 8:28:55, 0.28 it/s, Epoch 0.43/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>81.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>82.570500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>81.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>80.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>80.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>82.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>81.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>80.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>78.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>80.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>82.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>81.778200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>80.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>81.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>81.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>79.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>78.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>80.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>80.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>78.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>79.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>78.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>80.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>78.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>78.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>77.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>78.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>74.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>77.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>73.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>75.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>75.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>73.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>73.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>72.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>71.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>71.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>69.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>68.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>67.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>68.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>65.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>65.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>66.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>62.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>61.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>58.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>64.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>57.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>59.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>56.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>57.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>54.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>50.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>48.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>46.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>51.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>44.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>43.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>44.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>42.710500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>39.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>38.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>37.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>35.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>36.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>34.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>34.431700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>34.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>31.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>29.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>25.772100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>22.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>22.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>21.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>20.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>18.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>16.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>16.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>14.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>12.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>12.790100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>13.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>9.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>12.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>9.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>8.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>9.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>8.673600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>7.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>7.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>7.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>7.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>6.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>6.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>6.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>6.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>7.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>5.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>7.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>6.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>5.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>5.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>5.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>5.486700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>6.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>5.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>5.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>5.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>5.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>5.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>5.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>4.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>4.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>5.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>4.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>5.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>4.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>4.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>5.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>4.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>4.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>4.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>4.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>4.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>4.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>5.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>4.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>4.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>4.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>5.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>4.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>4.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>4.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>4.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>4.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>4.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>4.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>3.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>4.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>4.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>3.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>3.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>4.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>4.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.475700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>3.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>3.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>4.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>4.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.854900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>3.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>3.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>3.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>4.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>3.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>3.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>4.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>4.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>3.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.629900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>4.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>3.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>4.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>3.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>4.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>4.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>3.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>4.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>3.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>3.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>4.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>4.312600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>3.346600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>3.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>3.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>3.726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>3.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>3.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>3.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>3.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>2.924400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>3.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>3.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>3.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.875900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>3.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.199800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>3.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>3.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>2.860300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>2.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>2.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>2.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>2.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>2.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>3.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>2.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>3.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>2.756500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>2.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.867700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>1.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>1.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>1.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>2.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>2.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>1.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>1.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>1.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>2.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>1.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>1.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>1.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>1.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>1.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>2.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>1.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>1.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>1.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>1.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>1.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.822700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>1.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>1.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>1.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.611600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>1.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>1.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>1.725700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>1.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>1.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>1.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>1.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.594100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.869600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>2.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>1.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>1.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>1.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>1.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>0.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>1.816800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>1.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>1.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.867800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>1.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>1.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>0.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>1.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>0.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.772300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.555900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>0.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>0.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>1.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>1.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>1.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>1.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.855700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>1.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>1.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>1.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>1.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>0.970800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>1.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>0.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>0.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>0.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>0.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>2.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>0.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>1.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>0.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>1.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>0.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>0.818700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>0.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>0.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>1.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>1.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>0.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>1.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>1.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>0.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>1.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>1.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>0.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>0.783700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>1.332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>1.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>1.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>0.853100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>1.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>0.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>0.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>0.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>1.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.565700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>0.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>1.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>0.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>0.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>0.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>1.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>1.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>0.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>0.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>1.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>0.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>1.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>0.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>0.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0.777200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>1.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>0.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>0.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>0.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>0.777700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>0.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>1.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>0.569800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>0.866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>1.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>1.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>1.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>1.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>1.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>0.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.839500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>1.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>1.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>1.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>1.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>0.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>1.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>1.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>0.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>0.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>0.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>0.723900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>0.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>0.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>1.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>0.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>0.705500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>0.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>1.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>0.732800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>0.710500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>0.806200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>0.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>1.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>1.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>0.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>1.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>1.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>1.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>0.628100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.775100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>0.857400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>0.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>0.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>1.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>0.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>0.845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>1.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>0.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>0.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>0.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>1.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>1.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>0.528400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>0.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>0.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>1.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>0.567800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>0.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>1.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>0.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>0.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>0.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>0.819900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>1.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>1.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>0.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>1.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>1.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>1.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>0.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>1.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>0.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>0.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>0.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>0.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>1.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>0.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>0.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>1.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>0.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>1.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>1.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>0.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>0.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>0.731500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>0.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>1.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>1.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>0.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>0.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>0.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>0.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>1.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>0.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>1.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>0.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>0.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>0.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>0.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>1.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>0.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>1.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>0.797500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>0.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>0.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>0.831200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>0.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>0.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>0.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>1.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>1.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>0.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>0.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>0.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>0.623200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>0.821800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.790100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>0.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>0.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>1.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>0.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>0.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>1.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>0.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>1.479800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>1.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>0.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>0.976400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>0.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>0.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>1.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>0.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>0.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>0.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>0.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>0.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>0.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>0.839500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>1.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>0.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>0.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>0.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>1.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>0.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>1.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>0.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>0.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>0.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>0.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>0.436400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ") \n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=config[\"wandb_project\"], config=config)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    warmup_ratio=config[\"warmup_steps\"],\n",
    "    logging_steps=config[\"logging_steps\"],\n",
    "    save_steps=config[\"save_steps\"],\n",
    "    bf16=config[\"bf16\"],\n",
    "    bf16_full_eval=config[\"bf16_full_eval\"],\n",
    "    optim=config[\"optim\"],\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    # gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Move model to GPU and wrap with DataParallel if multiple GPUs available\n",
    "if torch.cuda.is_available():\n",
    "    # Check if model is not already on CUDA\n",
    "    if not next(model.parameters()).is_cuda:\n",
    "        model = model.cuda()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        # Check if model isn't already wrapped with DataParallel\n",
    "        if not isinstance(model, torch.nn.DataParallel):\n",
    "            # Use DataParallel with explicit device IDs\n",
    "            model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "def stage_trainer(stage=0):\n",
    "    model.current_stage = stage\n",
    "    current_output_dir = f\"{config['output_dir']}_stage{stage}\"\n",
    "    training_args.output_dir = current_output_dir\n",
    "    training_args.num_train_epochs = 5\n",
    "        \n",
    "\n",
    "    # Load the Reasoning model configuration\n",
    "    dataset_ = dataset.map(\n",
    "        (lambda x: preprocess_function(\n",
    "            x, \n",
    "            detector=detector,\n",
    "            stages=stage, \n",
    "            eos_token=tokenizer.eos_token,\n",
    "            bos_token=tokenizer.bos_token,\n",
    "            language_config=language_config\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    dataset_ = dataset_.map(\n",
    "        (lambda x: tokenizer_function(\n",
    "            x, \n",
    "            tokenizer=tokenizer,\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        remove_columns=[\"input\", \"instruction\", \"output\", \"prompt\"]\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_[\"train\"]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    # Save checkpoints\n",
    "    for folder in os.listdir(current_output_dir):\n",
    "        if folder.startswith(\"checkpoint-\"):\n",
    "            checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "            if os.path.isdir(checkpoint_folder):\n",
    "                tokenizer.save_pretrained(checkpoint_folder)\n",
    "                # If using DataParallel, save the base model\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                model_to_save.save_pretrained(checkpoint_folder)\n",
    "\n",
    "# Run training stages\n",
    "for stage in range(config[\"stages\"] + 1):\n",
    "    stage_trainer(stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we done training, let's load our fine tuned model for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model(model_name = \"output_stage1/checkpoint-125\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model = LatentReasoningGemmaForCausalLM(config_=model_config)\n",
    "    model = model.from_pretrained(model_name)\n",
    "    model.tokenizer = tokenizer\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    return  model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(model_name= \"output_stage1/checkpoint-125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Write me a poem about Machine Learning. \n",
      " ==========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.AnswerAnswer: 100.0%\n",
      "<answer>答え：インインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインインイン\n",
      "Outputs: ========================\n",
      "Write me a poem about Machine Learning.Answer: English English English English: 100.0%\n",
      "答え：ディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディ\n",
      "TOTAL TIME ELAPSED: 36.53s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Translate 'Hello, how are you?' to Japanese. \n",
      " ==========================================\n",
      "<bos>Translate 'Hello, how are you?' to Japanese.AnswerAnswer: 100.0%\n",
      "<answer>答え：ディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディ\n",
      "Outputs: ========================\n",
      "Translate 'Hello, how are you?' to Japanese.答え：100.0%\n",
      "答え：桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜\n",
      "TOTAL TIME ELAPSED: 40.26s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 「お元気ですか」を英語に訳すと  \n",
      " ==========================================\n",
      "<bos>「お元気ですか」を英語に訳すと AnswerAnswer: English English: 100.0%\n",
      "<answer>答え：桜桜桜<eos>\n",
      "Outputs: ========================\n",
      "「お元気ですか」を英語に訳すと Answer: English: 100.0%\n",
      "答え：ディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディ\n",
      "TOTAL TIME ELAPSED: 33.35s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Translate to english `「ねえ、それは何のためにあるの？` \n",
      " ==========================================\n",
      "<bos>Translate to english `「ねえ、それは何のためにあるの？`AnswerAnswer: English English English English: 100.0%\n",
      "<answer>答え：ディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディ\n",
      "Outputs: ========================\n",
      "Translate to english `「ねえ、それは何のためにあるの？`Answer: 100.0%\n",
      "答え：ディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディ\n",
      "TOTAL TIME ELAPSED: 40.17s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？` \n",
      " ==========================================\n",
      "<bos>「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？`AnswerAnswer: 100.0%\n",
      "<answer>答え：桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜\n",
      "Outputs: ========================\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？`AnswerAnswer: 100.0%\n",
      "答え：桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜\n",
      "TOTAL TIME ELAPSED: 31.71s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 格闘家ボブ・サップの出身国はどこでしょう？ \n",
      " ==========================================\n",
      "<bos>格闘家ボブ・サップの出身国はどこでしょう？AnswerAnswer: 100.0%\n",
      "<answer>答え：桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜桜\n",
      "Outputs: ========================\n",
      "格闘家ボブ・サップの出身国はどこでしょう？Answer: 100.0%\n",
      "答え：ディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディディ\n",
      "TOTAL TIME ELAPSED: 41.03s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_gen(\"Write me a poem about Machine Learning.\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"Translate 'Hello, how are you?' to Japanese.\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"「お元気ですか」を英語に訳すと \", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"Translate to english `「ねえ、それは何のためにあるの？`\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？`\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"格闘家ボブ・サップの出身国はどこでしょう？\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8) Evaulation\n",
    "\n",
    "Our evaluation framework employs multiple metrics to provide a thorough assessment of model performance, going beyond simple exact matching to capture various aspects of answer quality. But before we do, we need to preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/featurize/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 5032.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_eval_dataset_function(\n",
    "    examples, \n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the input examples by constructing the prompt with reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input examples with keys \"instruction\", \"input\", and \"output\".\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    new_inputs = []\n",
    "    \n",
    "    for i in range(len(instructions)):\n",
    "        instruction = instructions[i]\n",
    "        input = inputs[i]\n",
    "\n",
    "        input = instruction + input\n",
    "        new_inputs.append(input)\n",
    "\n",
    "\n",
    "    return {\"input\": new_inputs, \"output\": outputs, \"instructions\": instructions}\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess eval dataset\n",
    "eval_dataset_ = eval_dataset.map(\n",
    "    preprocess_eval_dataset_function,\n",
    "    batched=True,\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "We implement several complementary metrics to evaluate model performance:\n",
    "\n",
    "- Fuzzy Matching Accuracy\n",
    "\n",
    "    - Uses the FuzzyWuzzy algorithm to compute string similarity\n",
    "    - Accounts for minor variations in text (e.g., spacing, capitalization)\n",
    "    - Considers answers correct when similarity exceeds a configured threshold\n",
    "\n",
    "\n",
    "- ROUGE Scores\n",
    "\n",
    "    - ROUGE-1: Measures unigram overlap between predicted and target answers\n",
    "    - ROUGE-2: Measures bigram overlap, capturing phrase-level similarity\n",
    "    - Particularly effective for evaluating longer answers\n",
    "\n",
    "\n",
    "- BLEU Score\n",
    "\n",
    "    - Evaluates the precision of n-gram matches\n",
    "    - Provides a complementary perspective to ROUGE metrics\n",
    "    - Useful for assessing translation quality aspects of the answers\n",
    "\n",
    "\n",
    "- BERTScore\n",
    "\n",
    "    - Leverages contextual embeddings to capture semantic similarity\n",
    "    - More robust to paraphrasing than n-gram based metrics\n",
    "    - Correlates well with human judgments\n",
    "\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735437159/uzbhzyhmkhyegetyyrmg.png\" alt=\"https://ritikjain51.medium.com/llms-fine-tuning-and-evaluation-f019515b1c67\" width=\"400\"/>\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735437159/na4vh9ek8yksqes02qrd.png\" alt=\"https://ritikjain51.medium.com/llms-fine-tuning-and-evaluation-f019515b1c67\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, List, Union\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from thefuzz import fuzz\n",
    "from bert_score import score as bert_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetrics:\n",
    "    accuracy: float\n",
    "    avg_fuzzy_score: float\n",
    "    avg_bleu_score: float\n",
    "    avg_rouge1_f1: float\n",
    "    avg_rouge2_f1: float\n",
    "    avg_bert_score_f1: float\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            'accuracy': self.accuracy,\n",
    "            'avg_fuzzy_score': self.avg_fuzzy_score,\n",
    "            'avg_bleu_score': self.avg_bleu_score,\n",
    "            'avg_rouge1_f1': self.avg_rouge1_f1,\n",
    "            'avg_rouge2_f1': self.avg_rouge2_f1,\n",
    "            'avg_bert_score_f1': self.avg_bert_score_f1\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_answer_from_predicted_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text after '答え：' or 'Answer:' from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the answer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted answer, or an empty string if no match is found.\n",
    "    \"\"\"\n",
    "    prefixes = [\"答え：\", \"Answer:\"]\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        if prefix in text:\n",
    "            return text.split(prefix, 1)[1].strip()\n",
    "    \n",
    "    return text.strip()  # Return stripped text if no prefix found\n",
    "\n",
    "def compute_metrics(pred_answer: str, target_answer: str, threshold: int = 80) -> Dict[str, Union[float, bool]]:\n",
    "    \"\"\"\n",
    "    Compute multiple evaluation metrics for comparing predicted and target answers.\n",
    "    \"\"\"\n",
    "    # Preprocess answers\n",
    "    pred_clean = extract_answer_from_predicted_answer(pred_answer)\n",
    "    target_clean = target_answer.strip()\n",
    "    \n",
    "    # Convert to lowercase for consistent comparison\n",
    "    pred_lower = pred_clean.lower()\n",
    "    target_lower = target_clean.lower()\n",
    "    \n",
    "    # Calculate fuzzy match score\n",
    "    fuzzy_score = fuzz.ratio(pred_lower, target_lower)\n",
    "    \n",
    "    # Tokenize for BLEU score\n",
    "    pred_tokens = word_tokenize(pred_lower)\n",
    "    target_tokens = word_tokenize(target_lower)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    try:\n",
    "        bleu = sentence_bleu([target_tokens], pred_tokens, weights=(1.0,))\n",
    "    except ZeroDivisionError:\n",
    "        bleu = 0.0\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(target_lower, pred_lower)\n",
    "    \n",
    "    # Calculate BERTScore\n",
    "    # Detect if the text contains Japanese characters\n",
    "    def contains_japanese(text):\n",
    "        # Hiragana (3040-309F), Katakana (30A0-30FF), Kanji (4E00-9FFF)\n",
    "        for char in text:\n",
    "            if ('\\u3040' <= char <= '\\u309F' or  # Hiragana\n",
    "                '\\u30A0' <= char <= '\\u30FF' or  # Katakana\n",
    "                '\\u4E00' <= char <= '\\u9FFF'):   # Kanji\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Set language based on content\n",
    "    lang = 'ja' if contains_japanese(target_clean) else 'en'\n",
    "    \n",
    "    # Calculate BERTScore with appropriate language model\n",
    "    P, R, F1 = bert_score([pred_clean], [target_clean], lang=lang, verbose=False)\n",
    "    bert_f1 = F1.item()\n",
    "    \n",
    "    return {\n",
    "        'fuzzy_match': fuzzy_score >= threshold,\n",
    "        'fuzzy_score': fuzzy_score,\n",
    "        'bleu_score': bleu,\n",
    "        'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2_f1': rouge_scores['rouge2'].fmeasure,\n",
    "        'bert_score_f1': bert_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our evaluation helper function. We loop through the batch, call the generate function to \n",
    "get the output from the model and then compare it with the dataset output using the fuzzy matcher.dataset\n",
    "If it is correct, we add it up to the list of correct responses, if not we do not.\n",
    "\n",
    "This is how we figure out the metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    dataloader,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    max_new_tokens: int,\n",
    "    threshold: int = 80,\n",
    ") -> EvaluationMetrics:\n",
    "    \"\"\"\n",
    "    Evaluate the model using multiple metrics.\n",
    "    \n",
    "    Returns:\n",
    "        EvaluationMetrics: Object containing all computed metrics\n",
    "    \"\"\"\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Initialize metric aggregators\n",
    "    total_metrics = {\n",
    "        'fuzzy_score': 0,\n",
    "        'bleu_score': 0,\n",
    "        'rouge1_f1': 0,\n",
    "        'rouge2_f1': 0,\n",
    "        'bert_score_f1': 0\n",
    "    }\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        inputs = batch[\"input\"]\n",
    "        outputs = batch[\"output\"]\n",
    "        batch_size = len(inputs)\n",
    "        total_instances += batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            input_text = inputs[i]\n",
    "            target_answer = outputs[i]\n",
    "\n",
    "            # Generate the answer\n",
    "            pred_answer = generate_answer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                question=input_text,\n",
    "                max_length=max_new_tokens,\n",
    "            )\n",
    "\n",
    "            # Compute all metrics\n",
    "            metrics = compute_metrics(pred_answer, target_answer, threshold)\n",
    "            \n",
    "            # Update counters\n",
    "            if metrics['fuzzy_match']:\n",
    "                total_correct += 1\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += metrics[key]\n",
    "\n",
    "            if config[\"debug\"]:\n",
    "                pred_answer_extracted = extract_answer_from_predicted_answer(pred_answer)\n",
    "                print(\n",
    "                    f\"Input: {input_text}\\n\"\n",
    "                    f\"Target: {target_answer}\\n\"\n",
    "                    f\"Predicted: {pred_answer_extracted}\\n\"\n",
    "                    f\"Metrics: {metrics}\\n\"\n",
    "                )\n",
    "\n",
    "    # Calculate averages\n",
    "    accuracy = total_correct / total_instances\n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] /= total_instances\n",
    "\n",
    "    return EvaluationMetrics(\n",
    "        accuracy=accuracy,\n",
    "        avg_fuzzy_score=total_metrics['fuzzy_score'],\n",
    "        avg_bleu_score=total_metrics['bleu_score'],\n",
    "        avg_rouge1_f1=total_metrics['rouge1_f1'],\n",
    "        avg_rouge2_f1=total_metrics['rouge2_f1'],\n",
    "        avg_bert_score_f1=total_metrics['bert_score_f1']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(eval_dataset_[\"train\"], batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "def test_evaluation(model, tokenizer):\n",
    "    metrics = evaluate(dataloader, tokenizer, model, config[\"max_length\"])\n",
    "    print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.30s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？答え答え：ABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABCABC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/featurize/nltk_data'\n    - '/environment/miniconda3/nltk_data'\n    - '/environment/miniconda3/share/nltk_data'\n    - '/environment/miniconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint-75\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model(model_name \u001b[38;5;241m=\u001b[39m model_name)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m, in \u001b[0;36mtest_evaluation\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_evaluation\u001b[39m(model, tokenizer):\n\u001b[0;32m----> 6\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, tokenizer, model, max_new_tokens, threshold)\u001b[0m\n\u001b[1;32m     38\u001b[0m pred_answer \u001b[38;5;241m=\u001b[39m generate_answer(\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     40\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     41\u001b[0m     question\u001b[38;5;241m=\u001b[39minput_text,\n\u001b[1;32m     42\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute all metrics\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Update counters\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuzzy_match\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[27], line 68\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred_answer, target_answer, threshold)\u001b[0m\n\u001b[1;32m     65\u001b[0m fuzzy_score \u001b[38;5;241m=\u001b[39m fuzz\u001b[38;5;241m.\u001b[39mratio(pred_lower, target_lower)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Tokenize for BLEU score\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m pred_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_lower\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m target_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(target_lower)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/featurize/nltk_data'\n    - '/environment/miniconda3/nltk_data'\n    - '/environment/miniconda3/share/nltk_data'\n    - '/environment/miniconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# config[\"debug\"] = False\n",
    "for i in range(config[\"stages\"] + 1):\n",
    "    model_name = f\"output_stage{i}/checkpoint-125\"\n",
    "    model, tokenizer = load_model(model_name = model_name)\n",
    "    test_evaluation(model, tokenizer=tokenizer)\n",
    "    print(f\"Model : {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Model to Kaggle Models\n",
    "\n",
    "Step 1:  Turn the path to checkpoint to zip\n",
    "\n",
    "`zip -r latentreasoninggemma.zip /home/featurize/work/gemini-fine-tuning/output_stage3/checkpoint-1000`\n",
    "\n",
    "- Now, upload the .zip file to Kaggle Models.\n",
    "\n",
    "    - Step 1: Go to Kaggle Models\n",
    "    - Log in to your Kaggle account.\n",
    "    - Navigate to the Kaggle Models page.\n",
    "\n",
    "Step 2: Create a New Model\n",
    "    - Click on the \"New Model\" button.\n",
    "\n",
    "    - Fill in the required details:\n",
    "\n",
    "        - Model Name: LatentReasoningGemma2\n",
    "\n",
    "        - Description: Chain of continous thought . Include usage\n",
    "\n",
    "        - Visibility: public\n",
    "\n",
    "        - Click \"Create.\"\n",
    "\n",
    "Step 3: Upload the .zip File\n",
    "    - Click on the \"Upload Version\" button.\n",
    "\n",
    "    - Upload the zip file you created earlier.\n",
    "\n",
    "    - Add a version\n",
    "\n",
    "    - Click \"Upload.\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
